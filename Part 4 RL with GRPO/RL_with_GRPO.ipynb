{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -U \"unsloth>=2025.11.2\" \"transformers==4.56.2\" \"trl==0.22.2\" \"peft>=0.17.1\" \"datasets\" \"accelerate\" \"bitsandbytes\" \"torchvision\" \"pillow\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xtUFSuwQ-RJU",
    "outputId": "b2788b46-0b87-484d-9ddf-b1a461edad60"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting unsloth>=2025.11.2\n",
      "  Downloading unsloth-2025.11.3-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.56.2\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl==0.22.2\n",
      "  Downloading trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft>=0.17.1 in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Collecting peft>=0.17.1\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Collecting pillow\n",
      "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.1)\n",
      "Collecting unsloth_zoo>=2025.11.4 (from unsloth>=2025.11.2)\n",
      "  Downloading unsloth_zoo-2025.11.4-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (0.45.1)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (2.8.0+cu126)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (5.9.5)\n",
      "Collecting tyro (from unsloth>=2025.11.2)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (5.29.5)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth>=2025.11.2)\n",
      "  Downloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (3.4.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (0.2.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth>=2025.11.2) (0.35.2)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Collecting torch>=2.4.0 (from unsloth>=2025.11.2)\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth>=2025.11.2) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth>=2025.11.2) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth>=2025.11.2) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth>=2025.11.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth>=2025.11.2) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth>=2025.11.2) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth>=2025.11.2) (0.7.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth>=2025.11.2)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth>=2025.11.2)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2.5.0)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.11.4->unsloth>=2025.11.2)\n",
      "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.11.4->unsloth>=2025.11.2)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.11.4->unsloth>=2025.11.2)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth>=2025.11.2)\n",
      "  Downloading xformers-0.0.33-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "  Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "  Downloading xformers-0.0.29.post3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "INFO: pip is still looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "  Downloading xformers-0.0.29.post1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.29-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.28.post3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading xformers-0.0.28.post1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.28-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "  Downloading xformers-0.0.27.post2-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting torch>=2.4.0 (from unsloth>=2025.11.2)\n",
      "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth>=2025.11.2)\n",
      "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth>=2025.11.2) (8.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth>=2025.11.2) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth>=2025.11.2) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth>=2025.11.2)\n",
      "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth>=2025.11.2) (4.4.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth>=2025.11.2) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth>=2025.11.2) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth>=2025.11.2) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth>=2025.11.2) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth>=2025.11.2) (3.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth>=2025.11.2) (0.1.2)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.22.2-py3-none-any.whl (544 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth-2025.11.3-py3-none-any.whl (353 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m353.0/353.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m704.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.11.4-py3-none-any.whl (283 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.5/283.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
      "Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, triton, shtab, pyarrow, pillow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, tyro, nvidia-cusolver-cu12, transformers, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, peft, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.3.0\n",
      "    Uninstalling pillow-11.3.0:\n",
      "      Successfully uninstalled pillow-11.3.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvshmem-cu12\n",
      "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
      "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
      "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cu126\n",
      "    Uninstalling torchvision-0.23.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.23.0+cu126\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.17.1\n",
      "    Uninstalling peft-0.17.1:\n",
      "      Successfully uninstalled peft-0.17.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.0 which is incompatible.\n",
      "gradio 5.49.1 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 peft-0.18.0 pillow-12.0.0 pyarrow-22.0.0 shtab-1.8.0 torch-2.9.0 torchao-0.14.1 torchvision-0.24.0 transformers-4.56.2 triton-3.5.0 trl-0.22.2 tyro-0.9.35 unsloth-2025.11.3 unsloth_zoo-2025.11.4 xformers-0.0.33.post1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL"
        ]
       },
       "id": "745ef66fe11c425b858ddbe84353df1c"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, platform, torch\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"   # avoid wandb popups\n",
    "\n",
    "# IMPORT UNSLOTH FIRST (very important)\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"Python version :\", sys.version)\n",
    "print(\"Platform       :\", platform.platform())\n",
    "\n",
    "model_name = \"unsloth/smollm2-135m\"\n",
    "max_seq_len = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_len,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== Smoke test: ===\")\n",
    "text = \"Say hello in one short line.\"\n",
    "out = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "    max_new_tokens=15,\n",
    "    do_sample=False\n",
    ")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nâœ… Step 1 complete â€” environment OK & model loaded.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944,
     "referenced_widgets": [
      "668fe489fe1c4294a099a29f11fc9101",
      "e483aed1207644859f6127e3857aac19",
      "4b3bf67e00e9446cacb0c3201b8b5fb7",
      "5586f90a1f4149ea8c70c142f0ce839d",
      "a3772a06107b400bbf22de1cf8eb9931",
      "a7d6deb0498249ae9872dde3ad7a70cb",
      "27a909172d4b4b5cb85f251fba3b8484",
      "049ddd90c17d4a8d8af4efc2ebf7809a",
      "6299389b93f147249e3cdc8725d3878c",
      "ca5bfcfff0784917bf6c5ff7c5238646",
      "7fb7e476fbab461c8dc72e12de9605cb",
      "1e225e1a6e714cd9a17eb784b9afe0de",
      "889764c599354e2aa88a3c2950bcc148",
      "da10336766c84a3ea7f22a082c7eaa8d",
      "2a0b097b3288486bb2102f66ee4db97d",
      "78a87fa11ae64259a477ca0902b4b10b",
      "4b04b0b5bb2f476d949d01004cb727e9",
      "09dc858e99dd40ec94cab2156bbb96c7",
      "8ac362d0f3c04fae9b8b42d597c5503d",
      "a5ea92ec562245d6b4301efe2fec1d17",
      "ac1cb75c9e7b49fda82dcf578dddb1a8",
      "f4dd2983a08e4d05941ea04f3da2ce14",
      "46400c748c484588ba092d2e69700bf8",
      "c8b128b89bd34a1f8ff977e7f46a5c5c",
      "736c6358c57c4e789a3687252630e63a",
      "7b56a53278aa46ffbd1ac4e421f00702",
      "d808b540dcb3440b95ac0d81c360ce59",
      "65ea39fcc3244df99c63efce00dfdc07",
      "068916f7d7f74e8fb8f044915a9f050f",
      "d0d148ab33fb4d639254395ba8cf3573",
      "6d4b23e8c438436c98beef0966e53e72",
      "b3c4fd124bcd4964a491d16d86e6e5bc",
      "7042f4bd2b7c4a94a2fb0f69128f0d92",
      "8b7baee61e41402487df02fc7012c386",
      "c4da8e2f10ff4b7cb7627524d9e55686",
      "356e20d8433840f6aeca09ac2a56e59a",
      "c03b1a011b5c46028152478aabb29d6d",
      "bf28326e37634d59a04a0ad64bb36bd5",
      "b4ae30b54386480e84c3a9457a2dc65a",
      "ee9c913747064bf1a93b10290299091c",
      "0f8f82335f044326b17144599e794b31",
      "a03f71242387427c8afafaf45642d72d",
      "f503deb748c84901a6857ac2d531c671",
      "88e4163cd84748c39b376f6d11b667dd",
      "538f54d097f747ddb9f5ce57fb0c57e9",
      "8a77ff2caac24992920127a5f5d19521",
      "b5cea841c35a496ead30a08a79314762",
      "0f35af0ed16244a0a18c75c35dab2dd1",
      "7109c9d562e646cdbd82abc791ff21ee",
      "82924d1ea7884225b26da78869632b73",
      "1203f4419dd54462a84e6cac9abb2bee",
      "ce1f9df7691b46df8a081065d3a5a41e",
      "2f7855387c1f4dcbb66788a6597ae3b5",
      "13ddcb7eca2d4315b558684e7c250913",
      "8391c288d97a4a1bbf1b0576cb256585",
      "42388c13c70447029f988420f2f593a4",
      "4bf0dfac74ff4a06ab29dd08ef3ef185",
      "bba5b0ca88b040599fd54280778d0167",
      "cb390c22fe8c466a9fabf0a92ad61876",
      "2f3c29ee05a348b0b2a2be581f29426c",
      "222405ba5d7d4125922edcfa34098acc",
      "01e0645d79614e62b76b38616a88e880",
      "b45e0bd61cf74101bdb0c06b9c6893e4",
      "a5872c027a4445e18525d5f0d279b582",
      "5dee10c25b0a4acbb40ab4d7fbb64811",
      "46228c5f26de4429ac47ae6a4c0d65b8",
      "b1d875bc24fe4c04b12eed9720e87c3b",
      "6b46800ff99446898a310b2aa2860611",
      "6c4fc4f3fdf746a6b5b9e644179757cf",
      "7091a4ed8b6e4844be8cdc6d8cd2991c",
      "646408df85f04b61acc9477908632cc4",
      "ebf846d0acaa41549acc8d28eabe6b09",
      "d22bdd99009c4bef899734b403820276",
      "401323c3f653495ba9fa5ad03346227c",
      "78e5b26614d342219d8ea6c799a7a6d1",
      "a819a8490d37437e942b7fb999b99f67",
      "fb4f1037da854401930324a84aa9039b",
      "2e459a757daa410891cd3301d4d33764",
      "0f8ce9137eb64034908624a6c7ea071a",
      "460c727e8e0f4962857faf0b117d995a",
      "af24813f6e9545d1be7713d68caf6fa7",
      "80206e323db04ecdb0918627bdb35f2b",
      "face5e560de54bf39ab9dc363232f0f7",
      "16e063396b674531aa26dd13b10e0cef",
      "235d7c6121454d72a358ff3614d918a9",
      "42e577e562804ef3b0be81b39574fedb",
      "93c4c5e04d854a9696311c9a9794b791",
      "1502db6b9653403abf16efdfaca61c2d"
     ]
    },
    "id": "d0RebONk_KFR",
    "outputId": "243a7cf2-01cc-4275-e080-ade6f73af6d5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python version : 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "Platform       : Linux-6.6.105+-x86_64-with-glibc2.35\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/742 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Smoke test: ===\n",
      "Say hello in one short line.\n",
      "\n",
      "The first line of the program is the name of the program.\n",
      "\n",
      "âœ… Step 1 complete â€” environment OK & model loaded.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install --upgrade uv\n",
    "!uv pip install -q --upgrade \\\n",
    "  \"unsloth>=2025.11.2\" \\\n",
    "  \"transformers==4.56.2\" \\\n",
    "  \"trl==0.22.2\" \\\n",
    "  \"peft>=0.17.1\" \\\n",
    "  \"accelerate\" \"datasets\" \"bitsandbytes\" \"torchvision\" \"pillow\"\n",
    "\n",
    "# Import Unsloth BEFORE transformers/peft so the kernels patch correctly\n",
    "import sys, platform, torch, os\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"Python      :\", sys.version.split()[0])\n",
    "print(\"Platform    :\", platform.platform())\n",
    "\n",
    "# (Optional) Silence W&B prompts in Colab runs\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ---- Choose a tiny model that fits easily on T4\n",
    "model_name     = \"unsloth/smollm2-135m\"\n",
    "max_seq_length = 1024   # Plenty for simple GRPO toy tasks; you can raise later\n",
    "dtype          = torch.float16  # We'll train LoRA in 16-bit later\n",
    "\n",
    "# Load the base model (NOT 4-bit; we'll attach LoRA/GRPO later)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit   = False,        # keep real FP16 base for merging/adapters later\n",
    "    dtype          = dtype,\n",
    "    device_map     = \"auto\",\n",
    ")\n",
    "\n",
    "# ---- Quick smoke test\n",
    "text = \"You are a helpful assistant. Say hello in one short sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Smoke test output ===\")\n",
    "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n",
    "\n",
    "print(f\"\\nâœ… Step 1 ready: environment OK and base model loaded for GRPO.\\nPlanned model: {model_name}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sZZaRc1_6CS",
    "outputId": "457b9211-16cb-4140-fc8b-8f29ecdbee5a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python      : 3.12.12\n",
      "Platform    : Linux-6.6.105+-x86_64-with-glibc2.35\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "=== Smoke test output ===\n",
      "You are a helpful assistant. Say hello in one short sentence.\n",
      "\n",
      "You can also use the word â€œhelloâ€ in a sentence.\n",
      "\n",
      "You can also use the word â€œhelloâ€ in a sentence.\n",
      "\n",
      "âœ… Step 1 ready: environment OK and base model loaded for GRPO.\n",
      "Planned model: unsloth/smollm2-135m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, platform\n",
    "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.platform())\n",
    "\n",
    "# Use uv for faster installs if available; fall back to pip.\n",
    "try:\n",
    "    import uv\n",
    "except Exception:\n",
    "    !pip -q install -U uv\n",
    "\n",
    "# Core libs (versions known to work together for GRPO examples)\n",
    "!uv pip install -q \\\n",
    "  \"unsloth>=2025.11.2\" \\\n",
    "  \"transformers==4.56.2\" \\\n",
    "  \"trl==0.22.2\" \\\n",
    "  \"peft>=0.17.1\" \\\n",
    "  \"datasets\" \"accelerate\"\n",
    "\n",
    "# Optional (uncomment if you want 4-bit later)\n",
    "# !uv pip install -q bitsandbytes xformers\n",
    "\n",
    "# 1) IMPORTANT: import Unsloth BEFORE transformers/peft so its kernels patch correctly\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch\n",
    "import transformers, datasets, accelerate, peft\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__,\n",
    "      \"| TRL:\", __import__(\"trl\").__version__,\n",
    "      \"| PEFT:\", peft.__version__)\n",
    "\n",
    "# 2) Load a tiny base model for quick checks (no LoRA, no 4-bit yet)\n",
    "model_name = \"unsloth/smollm2-135m\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name       = model_name,\n",
    "    max_seq_length   = 2048,\n",
    "    load_in_4bit     = False,          # keep False for now; weâ€™ll add quant/LoRA later\n",
    "    dtype            = torch.float16,  # real FP16\n",
    "    device_map       = \"auto\",\n",
    ")\n",
    "\n",
    "# 3) Minimal helper that works whether or not a chat_template is set\n",
    "def quick_ask(prompt: str) -> str:\n",
    "    if getattr(tokenizer, \"chat_template\", None):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        text = f\"User: {prompt}\\nAssistant:\"\n",
    "    out = model.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "        max_new_tokens=40, do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# 4) Smoke test\n",
    "print(\"\\n=== Smoke test output ===\")\n",
    "print(quick_ask(\"Say hello in one short sentence.\"))\n",
    "\n",
    "print(f\"\\nâœ… Step 1 ready: environment OK and base model loaded for GRPO.\\nPlanned model: {model_name}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZcI7PnCAJLs",
    "outputId": "08634010-1e7f-4bae-899d-6ee5315e0e74"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python: 3.12.12 | Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
      "Transformers: 4.56.2 | TRL: 0.22.2 | PEFT: 0.17.1\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "=== Smoke test output ===\n",
      "User: Say hello in one short sentence.\n",
      "Assistant: Hello, I'm a student.\n",
      "\n",
      "The above example is a good example of a sentence that is short and to the point.\n",
      "\n",
      "Sentences that are too long are often called long sentences\n",
      "\n",
      "âœ… Step 1 ready: environment OK and base model loaded for GRPO.\n",
      "Planned model: unsloth/smollm2-135m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Build a minimal GRPO dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1) Raw problems for testing GRPO (you can expand later)\n",
    "raw_prompts = [\n",
    "    \"What is 12 + 7? Think step by step.\",\n",
    "    \"If a car drives 60 km in 1 hour, how far in 4 hours?\",\n",
    "    \"What is 9 * 9? Explain your reasoning.\",\n",
    "    \"If you buy 3 apples and each costs $2, how much total?\",\n",
    "    \"What is the next number: 2, 4, 8, 16, ?\"\n",
    "]\n",
    "\n",
    "# 2) Convert each raw prompt â†’ minimal chat-formatted prompt string\n",
    "# This DOES NOT use tokenizer.chat_template (avoids previous ValueError)\n",
    "def to_chat_prompt(question: str):\n",
    "    return f\"User: {question}\\nAssistant:\"\n",
    "\n",
    "processed = [{\"prompt\": to_chat_prompt(p)} for p in raw_prompts]\n",
    "\n",
    "# 3) Build HF Dataset\n",
    "grpo_dataset = Dataset.from_list(processed)\n",
    "\n",
    "print(\"âœ… Step 2 complete â€” Prompt dataset ready.\")\n",
    "print(\"Dataset size:\", len(grpo_dataset))\n",
    "\n",
    "print(\"\\n--- Sample encoded prompt ---\\n\")\n",
    "print(grpo_dataset[0][\"prompt\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ox3OKWH8A3Gy",
    "outputId": "efe79ba6-943b-4071-bcb9-c592ba4fc29b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 2 complete â€” Prompt dataset ready.\n",
      "Dataset size: 5\n",
      "\n",
      "--- Sample encoded prompt ---\n",
      "\n",
      "User: What is 12 + 7? Think step by step.\n",
      "Assistant:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, platform, torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"Python      :\", sys.version.split()[0])\n",
    "print(\"Platform    :\", platform.platform())\n",
    "\n",
    "# 2) Load a small base model for GRPO (no chat template; weâ€™ll use plain strings)\n",
    "model_name = \"unsloth/smollm2-135m\"\n",
    "max_seq_len = 1024\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name         = model_name,\n",
    "    max_seq_length     = max_seq_len,\n",
    "    load_in_4bit       = False,       # LoRA needs 16-bit params for training\n",
    "    fast_inference     = False,       # vLLM not needed for training loop\n",
    "    device_map         = \"auto\",\n",
    ")\n",
    "\n",
    "# 3) Add LoRA adapters (needed for RL training)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                        = 16,\n",
    "    target_modules          = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha              = 32,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state            = 3407,\n",
    ")\n",
    "\n",
    "# 4) Tiny helper: ask without chat_template (avoids template-related ValueErrors)\n",
    "def quick_ask(prompt: str) -> str:\n",
    "    text = f\"User: {prompt}\\nAssistant:\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=48,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# 5) Smoke test\n",
    "print(\"\\n=== Smoke test output ===\")\n",
    "print(quick_ask(\"Say hello in one short sentence.\"))\n",
    "\n",
    "print(\"\\nâœ… Step 1 ready: environment OK and base model loaded for GRPO.\")\n",
    "print(\"Planned model:\", model_name)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFBMfWaoBtcV",
    "outputId": "847761fe-0de7-4a1c-d211-635dca05825d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python      : 3.12.12\n",
      "Platform    : Linux-6.6.105+-x86_64-with-glibc2.35\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth 2025.11.2 patched 30 layers with 30 QKV layers, 30 O layers and 30 MLP layers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Smoke test output ===\n",
      "User: Say hello in one short sentence.\n",
      "Assistant: Hello, I'm a student.\n",
      "\n",
      "The above example is a good example of a sentence that is short and to the point.\n",
      "\n",
      "The sentence above is a good example of a sentence that is short and to the point.\n",
      "\n",
      "\n",
      "âœ… Step 1 ready: environment OK and base model loaded for GRPO.\n",
      "Planned model: unsloth/smollm2-135m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Build raw math prompts for GRPO\n",
    "\n",
    "raw_prompts = [\n",
    "    \"What is 12 + 7? Think step by step.\",\n",
    "    \"What is 5 + 9? Think step by step.\",\n",
    "    \"What is 20 - 3? Think step by step.\",\n",
    "    \"What is 4 * 6? Think step by step.\",\n",
    "    \"What is 32 / 4? Think step by step.\",\n",
    "]\n",
    "\n",
    "print(\"âœ… Step 2 ready â€” raw prompts collected.\")\n",
    "print(\"Number of prompts:\", len(raw_prompts))\n",
    "print(\"Sample:\", raw_prompts[0])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmQFr_ycCB7P",
    "outputId": "3313c0ff-ac08-441f-c684-b89b7cb2302d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 2 ready â€” raw prompts collected.\n",
      "Number of prompts: 5\n",
      "Sample: What is 12 + 7? Think step by step.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert raw prompts â†’ chat-formatted prompt strings\n",
    "\n",
    "def to_chat_prompt(question: str) -> str:\n",
    "    return f\"User: {question}\\nAssistant:\"\n",
    "\n",
    "processed = [{\"prompt\": to_chat_prompt(p)} for p in raw_prompts]\n",
    "\n",
    "print(\"âœ… Step 3 complete â€” Chat-formatted prompts created.\")\n",
    "print(\"Dataset size:\", len(processed))\n",
    "print(\"\\n--- Sample chat prompt ---\\n\")\n",
    "print(processed[0][\"prompt\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FG3JJKm-COPf",
    "outputId": "b35ca3ff-2366-4cff-ebc3-1a2ff9929f91"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 3 complete â€” Chat-formatted prompts created.\n",
      "Dataset size: 5\n",
      "\n",
      "--- Sample chat prompt ---\n",
      "\n",
      "User: What is 12 + 7? Think step by step.\n",
      "Assistant:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "grpo_dataset = Dataset.from_list(processed)\n",
    "\n",
    "print(\"âœ… Step 4A complete â€” HF Dataset built.\")\n",
    "print(\"Dataset size:\", len(grpo_dataset))\n",
    "print(\"\\n--- First sample ---\\n\")\n",
    "print(grpo_dataset[0])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAa_zO4MCYeL",
    "outputId": "dc28ad92-a992-4fc7-d998-d6553c5f27f9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 4A complete â€” HF Dataset built.\n",
      "Dataset size: 5\n",
      "\n",
      "--- First sample ---\n",
      "\n",
      "{'prompt': 'User: What is 12 + 7? Think step by step.\\nAssistant:'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom GRPO Reward Function\n",
    "\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Extract first integer from a model completion.\"\"\"\n",
    "    numbers = re.findall(r\"-?\\d+\", text)\n",
    "    return int(numbers[0]) if numbers else None\n",
    "\n",
    "def reward_function(prompts, completions, completion_ids=None, **kwargs):\n",
    "    rewards = []\n",
    "\n",
    "    for prompt, answer in zip(prompts, completions):\n",
    "        # Compute correct answer from the prompt\n",
    "        try:\n",
    "            if \"12 + 7\" in prompt:\n",
    "                correct = 19\n",
    "            elif \"5 + 9\" in prompt:\n",
    "                correct = 14\n",
    "            elif \"20 - 3\" in prompt:\n",
    "                correct = 17\n",
    "            elif \"4 * 6\" in prompt:\n",
    "                correct = 24\n",
    "            elif \"32 / 4\" in prompt:\n",
    "                correct = 8\n",
    "            else:\n",
    "                correct = None\n",
    "        except:\n",
    "            correct = None\n",
    "\n",
    "        predicted = extract_number(answer)\n",
    "\n",
    "        if predicted is None or correct is None:\n",
    "            rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(1.0 if predicted == correct else 0.0)\n",
    "\n",
    "    return torch.tensor(rewards, dtype=torch.float32)\n"
   ],
   "metadata": {
    "id": "-pP09rlTCfMX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_prompts = [\"User: What is 12 + 7? Think step by step.\\nAssistant:\"]\n",
    "test_outputs = [\"The answer is 19.\"]\n",
    "print(\"Reward:\", reward_function(test_prompts, test_outputs))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MAbm8DKCiOo",
    "outputId": "59fe4a84-a380-407e-e60d-ff14e944f899"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reward: tensor([1.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, platform, torch\n",
    "# Import Unsloth FIRST so its patches apply correctly\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"Python      :\", sys.version.split()[0])\n",
    "print(\"Platform    :\", platform.platform())\n",
    "\n",
    "# 2) Load a tiny base model for GRPO demos\n",
    "max_seq_len = 2048\n",
    "model_name = \"unsloth/smollm2-135m\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_len,\n",
    "    fast_inference = False,   # âœ… important fix\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Minimal â€œchatâ€ helper that does NOT rely on tokenizer.chat_template\n",
    "#    (this avoids template/not-set errors later)\n",
    "def quick_ask(prompt: str, max_new_tokens: int = 64):\n",
    "    text = f\"User: {prompt}\\nAssistant:\"\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# 4) Smoke test â€” a one-line instruction\n",
    "print(\"\\n=== Smoke test output ===\")\n",
    "print( quick_ask(\"Say hello in one short sentence.\") )\n",
    "\n",
    "print(f\"\\nâœ… Step 1 ready: environment OK and base model loaded for GRPO.\")\n",
    "print(f\"Planned model: {model_name}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOTWKMNbC5q4",
    "outputId": "fe57dcaa-20bc-493e-ac53-43c652b7d0ad"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python      : 3.12.12\n",
      "Platform    : Linux-6.6.105+-x86_64-with-glibc2.35\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "=== Smoke test output ===\n",
      "User: Say hello in one short sentence.\n",
      "Assistant: I am a student of the University of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City of the City\n",
      "\n",
      "âœ… Step 1 ready: environment OK and base model loaded for GRPO.\n",
      "Planned model: unsloth/smollm2-135m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Build raw prompts for GRPO\n",
    "\n",
    "raw_prompts = [\n",
    "    \"What is 12 + 7? Think step by step.\",\n",
    "    \"What is 5 + 9? Think step by step.\",\n",
    "    \"What is 20 - 3? Think step by step.\",\n",
    "    \"What is 4 * 6? Think step by step.\",\n",
    "    \"What is 32 / 4? Think step by step.\",\n",
    "]\n",
    "\n",
    "print(\"âœ… Step 2 ready â€” raw prompts collected.\")\n",
    "print(\"Number of prompts:\", len(raw_prompts))\n",
    "print(\"Sample:\", raw_prompts[0])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f65N-Lg0ELWL",
    "outputId": "0e9ed52d-b21c-4e9e-b320-8a2b20a87b0f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 2 ready â€” raw prompts collected.\n",
      "Number of prompts: 5\n",
      "Sample: What is 12 + 7? Think step by step.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert raw prompts\n",
    "def to_chat_prompt(question: str) -> str:\n",
    "    # Minimal chat-style formatting\n",
    "    return f\"User: {question}\\nAssistant:\"\n",
    "\n",
    "processed = [\n",
    "    {\"prompt\": to_chat_prompt(p)}\n",
    "    for p in raw_prompts\n",
    "]\n",
    "\n",
    "print(\"âœ… Step 3 complete â€” Chat-formatted prompts created.\")\n",
    "print(\"Dataset size:\", len(processed))\n",
    "print(\"\\n--- Sample chat prompt ---\\n\")\n",
    "print(processed[0][\"prompt\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqoXJB9-EUT2",
    "outputId": "ab0070ca-583d-436d-9265-cf7d84714d57"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 3 complete â€” Chat-formatted prompts created.\n",
      "Dataset size: 5\n",
      "\n",
      "--- Sample chat prompt ---\n",
      "\n",
      "User: What is 12 + 7? Think step by step.\n",
      "Assistant:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# (RE)ATTACH LORA ADAPTERS â€” required for training a quantized model\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# If you re-ran cells, it's easy to lose adapters; guard & reattach if needed\n",
    "needs_lora = not hasattr(model, \"peft_config\")\n",
    "if needs_lora:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,                              # 8/16/32 work; raise for a bit more capacity\n",
    "        target_modules=[\n",
    "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "            \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",  # saves VRAM\n",
    "        random_state=3407,\n",
    "    )\n",
    "\n",
    "# Optional: sanity check how many params will train\n",
    "try:\n",
    "    FastLanguageModel.print_trainable_parameters(model)\n",
    "except Exception:\n",
    "    pass\n"
   ],
   "metadata": {
    "id": "wqpRtfMXE0Nv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import GRPOTrainer, GRPOConfig  # or your existing imports\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    per_device_train_batch_size=4,         # multiple of num_generations\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_generations=4,\n",
    "    learning_rate=5e-6,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=128,\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,                 # GRPO doesnâ€™t need a reference model\n",
    "    reward_funcs=[reward_function], # your reward fn from Step 3\n",
    "    args=grpo_config,\n",
    "    train_dataset=grpo_dataset,     # your prompts dataset\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ],
   "metadata": {
    "id": "LDxGh4ZxFW6S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nğŸ”µ Starting GRPO training â€¦\")\n",
    "\n",
    "out = trainer.train()\n",
    "\n",
    "print(\"âœ… Training finished.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "GOKug1e9GBC3",
    "outputId": "a827b0a7-e6dd-4b4c-b250-3d1ed5f23cdf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ”µ Starting GRPO training â€¦\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5 | Num Epochs = 3 | Total steps = 6\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 4,884,480 of 139,400,064 (3.50% trained)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / reward_function / mean</th>\n",
       "      <th>rewards / reward_function / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>127.975000</td>\n",
       "      <td>127.800000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.070711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Training finished.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def make_prompt(a,b,op_sym):\n",
    "    expr = f\"{a} {op_sym} {b}\"\n",
    "    return {\"prompt\": f\"User: What is {expr}? Think step by step.\\nAssistant:\"}\n",
    "\n",
    "def build_rand_dataset(n=400):\n",
    "    ops = ['+','-','*','/']\n",
    "    rows=[]\n",
    "    for _ in range(n):\n",
    "        op_sym = random.choice(ops)\n",
    "        a = random.randint(1,50)\n",
    "        b = random.randint(1,50 if op_sym!='/' else 10)\n",
    "        if op_sym=='/' and a % b != 0:  # prefer integers for division reward\n",
    "            a = b * random.randint(1,10)\n",
    "        rows.append(make_prompt(a,b,op_sym))\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "grpo_dataset = build_rand_dataset(400)   # replace your 5 prompts\n",
    "print(grpo_dataset[0])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtOSlndcK08P",
    "outputId": "bf116f22-856d-4796-e9bb-faef0d440d75"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'prompt': 'User: What is 27 + 9? Think step by step.\\nAssistant:'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Deterministic single-turn generation\n",
    "import torch\n",
    "\n",
    "def ask(q: str, max_new_tokens: int = 24):\n",
    "    prompt = f\"User: {q}\\nAssistant:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,          # greedy for eval\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=1.02,\n",
    "            eos_token_id=getattr(tokenizer, \"eos_token_id\", None),\n",
    "            pad_token_id=getattr(tokenizer, \"pad_token_id\", getattr(tokenizer, \"eos_token_id\", None)),\n",
    "        )\n",
    "\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(gen, skip_special_tokens=True)\n",
    "    # keep only the first line to avoid the model continuing\n",
    "    return text.strip().splitlines()[0].strip()\n",
    "\n",
    "tests = [\n",
    "    \"What is 12 + 7? Think step by step.\",\n",
    "    \"What is 5 + 9? Think step by step.\",\n",
    "    \"What is 20 - 3? Think step by step.\",\n",
    "    \"What is 4 * 6? Think step by step.\",\n",
    "    \"What is 32 / 4? Think step by step.\",\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(\"Q:\", t)\n",
    "    print(\"A:\", ask(t))\n",
    "    print(\"-\" * 60)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hH7BMUQ2IQhj",
    "outputId": "a48db4be-53fa-409a-84f9-04e7f30daa01"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q: What is 12 + 7? Think step by step.\n",
      "A: 12 + 7 = 20\n",
      "------------------------------------------------------------\n",
      "Q: What is 5 + 9? Think step by step.\n",
      "A: 5 + 9 = 14\n",
      "------------------------------------------------------------\n",
      "Q: What is 20 - 3? Think step by step.\n",
      "A: 20 - 3 = 15\n",
      "------------------------------------------------------------\n",
      "Q: What is 4 * 6? Think step by step.\n",
      "A: 4 * 6 = 24\n",
      "------------------------------------------------------------\n",
      "Q: What is 32 / 4? Think step by step.\n",
      "A: 32 / 4 = 6\n",
      "------------------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "model.eval()\n",
    "\n",
    "def to_chat_prompt(q: str) -> str:\n",
    "    return f\"User: {q}\\nAssistant:\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_answer(question: str, max_new_tokens=64, temperature=0.0):\n",
    "    text = to_chat_prompt(question)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample = temperature > 0.0,\n",
    "        temperature = temperature if temperature > 0.0 else None,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is 12 + 7? Think step by step.\",\n",
    "    \"What is 5 + 9? Think step by step.\",\n",
    "    \"What is 20 - 3? Think step by step.\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(\"Q:\", q)\n",
    "    print(generate_answer(q, max_new_tokens=128))\n",
    "    print(\"-\"*80)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwvokBJ8G476",
    "outputId": "6da952cb-24c8-4041-fd69-6edc094ffb90"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q: What is 12 + 7? Think step by step.\n",
      "User: What is 12 + 7? Think step by step.\n",
      "Assistant: 12 + 7 = 21\n",
      "\n",
      "Question 2:\n",
      "\n",
      "What is 12 + 7 + 7? Think step by step.\n",
      "Assistant: 12 + 7 + 7 = 21\n",
      "\n",
      "Question 3:\n",
      "\n",
      "What is 12 + 7 + 7 + 7? Think step by step.\n",
      "Assistant: 12 + 7 + 7 + 7 = 21\n",
      "\n",
      "Question 4:\n",
      "\n",
      "What is 12 + 7 + 7 + 7 + 7? Think step\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What is 5 + 9? Think step by step.\n",
      "User: What is 5 + 9? Think step by step.\n",
      "Assistant: 5 + 9 = 14\n",
      "\n",
      "Question 2: What is 5 + 9 + 14? Think step by step.\n",
      "Assistant: 14 + 5 = 21\n",
      "\n",
      "Question 3: What is 5 + 9 + 14 + 21? Think step by step.\n",
      "Assistant: 21 + 5 = 25\n",
      "\n",
      "Question 4: What is 5 + 9 + 14 + 21 + 25? Think step by step.\n",
      "Assistant: 25 + 5\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What is 20 - 3? Think step by step.\n",
      "User: What is 20 - 3? Think step by step.\n",
      "Assistant: 20 - 3 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15 - 20 = 15\n",
      "\n",
      "- 15\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def make_prompt(expr: str) -> str:\n",
    "    return f\"User: {expr} Think step by step.\\nAssistant:\"\n",
    "\n",
    "ops = [\"+\", \"-\", \"*\", \"/\"]\n",
    "\n",
    "def rand_term():\n",
    "    return str(random.randint(1, 50))\n",
    "\n",
    "def rand_expr(max_terms=3):\n",
    "    k = random.randint(2, max_terms)  # number of terms (2..max_terms)\n",
    "    parts = [rand_term()]\n",
    "    for _ in range(k-1):\n",
    "        op = random.choice(ops)\n",
    "        # keep division nicer (avoid tiny fractions / zero)\n",
    "        if op == \"/\":\n",
    "            parts.append(\"/\")\n",
    "            parts.append(str(random.randint(1, 12)))\n",
    "        else:\n",
    "            parts.append(op)\n",
    "            parts.append(rand_term())\n",
    "    return \" \".join(parts)\n",
    "\n",
    "N = 1500  # bump up for more data if you have time\n",
    "raw_prompts = [f\"What is {rand_expr(max_terms=4)}?\" for _ in range(N)]\n",
    "\n",
    "# re-use your simple chat formatting (no chat_template needed)\n",
    "def to_chat_prompt(question: str) -> str:\n",
    "    return f\"User: {question} Think step by step.\\nAssistant:\"\n",
    "\n",
    "processed = [{\"prompt\": to_chat_prompt(p)} for p in raw_prompts]\n",
    "grpo_dataset = Dataset.from_list(processed)\n",
    "\n",
    "print(\"Dataset size:\", len(grpo_dataset))\n",
    "print(\"Sample prompt:\\n\", grpo_dataset[0][\"prompt\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YR-3PSTL_W9",
    "outputId": "90c307d8-da79-4122-f510-be43950b5ba6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset size: 1500\n",
      "Sample prompt:\n",
      " User: What is 34 + 17 - 26 / 5? Think step by step.\n",
      "Assistant:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re, torch\n",
    "from typing import List\n",
    "\n",
    "def safe_eval_arith(s: str):\n",
    "    # very small safe evaluator: digits, + - * / and spaces only\n",
    "    if not re.fullmatch(r\"[0-9+\\-*/\\s]+\", s):\n",
    "        return None\n",
    "    try:\n",
    "        return eval(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_expr_from_prompt(prompt: str):\n",
    "    # pull \"What is ... ?\" segment\n",
    "    m = re.search(r\"What is (.+?)\\?\", prompt)\n",
    "    if not m:\n",
    "        return None\n",
    "    return m.group(1).strip()\n",
    "\n",
    "def extract_last_number(text: str):\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return None if not nums else float(nums[-1])\n",
    "\n",
    "def reward_function(prompts: List[str], completions: List[str], **kwargs):\n",
    "    rewards = []\n",
    "    for prompt, out in zip(prompts, completions):\n",
    "        expr = extract_expr_from_prompt(prompt)\n",
    "        target = None if expr is None else safe_eval_arith(expr.replace(\"  \", \" \"))\n",
    "        pred = extract_last_number(out)\n",
    "        if target is None or pred is None:\n",
    "            rewards.append(0.0)\n",
    "        else:\n",
    "            # exact match for ints, close-match for divisions\n",
    "            if abs(pred - target) < 1e-3:\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                # small partial reward if close (helps stabilize early learning)\n",
    "                gap = min(abs(pred - target), 10.0)\n",
    "                rewards.append(max(0.0, 1.0 - gap/10.0) * 0.3)\n",
    "    return torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "# quick smoke test\n",
    "test_p = [\"User: What is 12 + 7? Think step by step.\\nAssistant:\"]\n",
    "test_c = [\"12 + 7 = 19\\nTherefore the answer is 19.\"]\n",
    "print(\"Reward smoke:\", reward_function(test_p, test_c))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTGkXPA3MCvp",
    "outputId": "433e9276-cfb3-496a-cdba-695311a1546c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reward smoke: tensor([1.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"  # disables torch.compile globally\n",
    "\n",
    "# (Optional) If your GRPO/TrainingArguments object supports it:\n",
    "# grpo_config = GRPOConfig(..., torch_compile=False)  # keep compile off at the trainer level\n"
   ],
   "metadata": {
    "id": "T3SqPK1AlAU8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    per_device_train_batch_size=4,      # multiple of num_generations\n",
    "    gradient_accumulation_steps=4,      # effective batch up\n",
    "    num_generations=4,                  # keep 4 for T4 memory\n",
    "    learning_rate=1e-5,                 # smaller LR for stability\n",
    "    num_train_epochs=1,                 # longer training\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    max_prompt_length=128,\n",
    "    max_completion_length=64,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,               # your LoRA-patched model from Step 1\n",
    "    ref_model=None,            # GRPO does not need a reference model\n",
    "    reward_funcs=[reward_function],\n",
    "    args=grpo_config,\n",
    "    train_dataset=grpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ”µ Starting GRPO training â€¦\")\n",
    "out = trainer.train()\n",
    "print(\"âœ… Training finished.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nK6tqi-rMFgb",
    "outputId": "d23a09ab-47f7-4d44-e1a6-230084f49924"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ”µ Starting GRPO training â€¦\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,500 | Num Epochs = 1 | Total steps = 375\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 4,884,480 of 139,400,064 (3.50% trained)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 45:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / reward_function / mean</th>\n",
       "      <th>rewards / reward_function / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028676</td>\n",
       "      <td>0.031263</td>\n",
       "      <td>63.437500</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.016480</td>\n",
       "      <td>0.028676</td>\n",
       "      <td>0.069634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045715</td>\n",
       "      <td>0.038633</td>\n",
       "      <td>62.806250</td>\n",
       "      <td>46.400000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>0.037339</td>\n",
       "      <td>0.045715</td>\n",
       "      <td>0.082727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.034113</td>\n",
       "      <td>63.281250</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.059571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022984</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>63.568750</td>\n",
       "      <td>57.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.022984</td>\n",
       "      <td>0.051660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042751</td>\n",
       "      <td>0.043344</td>\n",
       "      <td>62.975000</td>\n",
       "      <td>49.700000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>0.034055</td>\n",
       "      <td>0.042751</td>\n",
       "      <td>0.089769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026575</td>\n",
       "      <td>0.030059</td>\n",
       "      <td>63.431250</td>\n",
       "      <td>55.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>12.950000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>0.015138</td>\n",
       "      <td>0.026575</td>\n",
       "      <td>0.059861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036019</td>\n",
       "      <td>0.037743</td>\n",
       "      <td>63.137500</td>\n",
       "      <td>52.400000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>14.300000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>0.044724</td>\n",
       "      <td>0.036019</td>\n",
       "      <td>0.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032207</td>\n",
       "      <td>0.035336</td>\n",
       "      <td>63.643750</td>\n",
       "      <td>59.300000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.016416</td>\n",
       "      <td>0.032207</td>\n",
       "      <td>0.065899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041147</td>\n",
       "      <td>0.041411</td>\n",
       "      <td>63.443750</td>\n",
       "      <td>55.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.039186</td>\n",
       "      <td>0.041147</td>\n",
       "      <td>0.082796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>63.475000</td>\n",
       "      <td>55.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>0.016904</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>0.037836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>0.028279</td>\n",
       "      <td>63.318750</td>\n",
       "      <td>53.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>0.055348</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>0.061720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023287</td>\n",
       "      <td>0.029038</td>\n",
       "      <td>63.037500</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>0.037139</td>\n",
       "      <td>0.023287</td>\n",
       "      <td>0.047433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.045221</td>\n",
       "      <td>63.125000</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>0.017793</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.069239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050631</td>\n",
       "      <td>0.054482</td>\n",
       "      <td>63.193750</td>\n",
       "      <td>51.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>0.024842</td>\n",
       "      <td>0.050631</td>\n",
       "      <td>0.098689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059131</td>\n",
       "      <td>0.048371</td>\n",
       "      <td>63.231250</td>\n",
       "      <td>54.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>10.950000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>0.059131</td>\n",
       "      <td>0.087225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>63.262500</td>\n",
       "      <td>52.200000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>0.049288</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.076870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>0.018497</td>\n",
       "      <td>63.562500</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>0.044094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041054</td>\n",
       "      <td>0.043401</td>\n",
       "      <td>63.537500</td>\n",
       "      <td>56.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>0.015327</td>\n",
       "      <td>0.041054</td>\n",
       "      <td>0.088153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.034163</td>\n",
       "      <td>0.022238</td>\n",
       "      <td>62.218750</td>\n",
       "      <td>39.800000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0.129664</td>\n",
       "      <td>0.034163</td>\n",
       "      <td>0.063684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033615</td>\n",
       "      <td>0.044440</td>\n",
       "      <td>63.650000</td>\n",
       "      <td>58.400000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.033615</td>\n",
       "      <td>0.067778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.037718</td>\n",
       "      <td>63.975000</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.070242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044498</td>\n",
       "      <td>0.049438</td>\n",
       "      <td>62.487500</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.943750</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>23.700000</td>\n",
       "      <td>0.048717</td>\n",
       "      <td>0.044498</td>\n",
       "      <td>0.085574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>0.025384</td>\n",
       "      <td>62.912500</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>0.053704</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>0.056074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.030347</td>\n",
       "      <td>0.029701</td>\n",
       "      <td>62.568750</td>\n",
       "      <td>45.900000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>0.117916</td>\n",
       "      <td>0.030347</td>\n",
       "      <td>0.059078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.035508</td>\n",
       "      <td>63.731250</td>\n",
       "      <td>59.700000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.060126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>63.343750</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.076805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>63.956250</td>\n",
       "      <td>63.300000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>0.062675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047991</td>\n",
       "      <td>0.048572</td>\n",
       "      <td>63.625000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.010997</td>\n",
       "      <td>0.047991</td>\n",
       "      <td>0.096794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047971</td>\n",
       "      <td>0.048494</td>\n",
       "      <td>63.787500</td>\n",
       "      <td>60.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>0.047971</td>\n",
       "      <td>0.093560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029896</td>\n",
       "      <td>0.032061</td>\n",
       "      <td>62.981250</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.029896</td>\n",
       "      <td>0.063317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053424</td>\n",
       "      <td>0.048837</td>\n",
       "      <td>62.925000</td>\n",
       "      <td>47.100000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>0.021712</td>\n",
       "      <td>0.053424</td>\n",
       "      <td>0.090431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>63.400000</td>\n",
       "      <td>54.400000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>0.054845</td>\n",
       "      <td>0.128071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022014</td>\n",
       "      <td>0.030062</td>\n",
       "      <td>63.043750</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>8.850000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.025701</td>\n",
       "      <td>0.022014</td>\n",
       "      <td>0.049177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.031446</td>\n",
       "      <td>63.675000</td>\n",
       "      <td>58.800000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.062570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.023102</td>\n",
       "      <td>0.024495</td>\n",
       "      <td>62.293750</td>\n",
       "      <td>42.300000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>11.350000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0.086154</td>\n",
       "      <td>0.023102</td>\n",
       "      <td>0.054979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035721</td>\n",
       "      <td>0.036477</td>\n",
       "      <td>63.512500</td>\n",
       "      <td>56.200000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>0.017420</td>\n",
       "      <td>0.035721</td>\n",
       "      <td>0.081332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067049</td>\n",
       "      <td>0.070037</td>\n",
       "      <td>63.631250</td>\n",
       "      <td>59.800000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.024142</td>\n",
       "      <td>0.067049</td>\n",
       "      <td>0.115407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Training finished.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def ask(q: str):\n",
    "    text = f\"User: {q} Think step by step.\\nAssistant:\"\n",
    "    out = model.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,   # greedy for correctness\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True).split(\"Assistant:\",1)[-1].strip()\n",
    "\n",
    "tests = [\n",
    "    \"What is 5 + 9?\",\n",
    "    \"What is 4 * 6?\",\n",
    "    \"What is 8 + 7 + 6?\",\n",
    "    \"What is 40 / 8 + 3?\",\n",
    "]\n",
    "for t in tests:\n",
    "    print(\"Q:\", t)\n",
    "    print(\"A:\", ask(t))\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29PoGfrZvf2E",
    "outputId": "2032e1b1-fd40-4c80-eda7-f2d668a747c1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q: What is 5 + 9?\n",
      "A: 5 + 9 = 14\n",
      "\n",
      "Step 2:\n",
      "\n",
      "Assistant: 5 + 9 = 14\n",
      "\n",
      "Step 3:\n",
      "\n",
      "Assistant: 14 = 14\n",
      "\n",
      "Step 4:\n",
      "\n",
      "Assistant: 14 = 1\n",
      "Q: What is 4 * 6?\n",
      "A: 4 * 6 = 24\n",
      "\n",
      "Question 2: What is 4 * 6/24? Think step by step.\n",
      "Assistant: 4 * 6/24 = 24/4 = 6\n",
      "\n",
      "Question 3: What is 4 *\n",
      "Q: What is 8 + 7 + 6?\n",
      "A: 8 + 7 + 6 = 15\n",
      "\n",
      "Step 2:\n",
      "\n",
      "Assistant: 15 + 8 = 21\n",
      "\n",
      "Step 3:\n",
      "\n",
      "Assistant: 21 + 7 = 28\n",
      "\n",
      "Step 4:\n",
      "\n",
      "Assistant\n",
      "Q: What is 40 / 8 + 3?\n",
      "A: 40 + 3 = 40 + 3\n",
      "\n",
      "Step 2: 40 - 3 = 30 - 3 = 27\n",
      "\n",
      "Step 3: 27 - 3 = 27 - 3 = 26\n",
      "\n",
      "Step 4\n"
     ]
    }
   ]
  }
 ]
}