{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip -q install --upgrade \\\n",
    "  unsloth datasets accelerate bitsandbytes wandb huggingface_hub \\\n",
    "  \"transformers==4.57.1\" \\\n",
    "  \"trl>=0.10.0\"\n",
    "\n",
    "# ---- Imports & versions\n",
    "import os, sys, random, numpy as np, torch\n",
    "print(\"Python        :\", sys.version.split()[0])\n",
    "print(\"NumPy         :\", np.__version__)\n",
    "print(\"Transformers  :\", __import__(\"transformers\").__version__)\n",
    "print(\"TRL           :\", __import__(\"trl\").__version__)\n",
    "\n",
    "# ---- CUDA / GPU info\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU           :\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version  :\", torch.version.cuda)\n",
    "    print(\"CC            :\", torch.cuda.get_device_capability(0))\n",
    "\n",
    "# ---- Reproducibility seeds\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ---- Default: disable W&B unless you enable it in Step 2\n",
    "os.environ.setdefault(\"WANDB_DISABLED\", \"true\")\n",
    "print(\"\\nâœ… Step 1 complete: installs + env OK. Ready for Step 2 (HF/W&B auth).\")"
   ],
   "metadata": {
    "id": "K6MvCGLtgVpr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Load SmolLM2  + attach LoRA\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"unsloth/smollm2-135m\"\n",
    "MAX_LEN    = 1024\n",
    "\n",
    "# Pick dtype (bf16 if supported, else fp16)\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "DTYPE    = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = MODEL_NAME,\n",
    "    max_seq_length = MAX_LEN,\n",
    "    dtype          = DTYPE,\n",
    "    load_in_4bit   = True,\n",
    "    token          = hf_token if \"hf_token\" in globals() and hf_token else None,\n",
    ")\n",
    "\n",
    "# Tokenizer hygiene\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Attach LoRA (pass kwargs; do NOT pass a LoraConfig object)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Quick generate smoke test\n",
    "_test = \"Classify the sentiment: I absolutely love this new feature!\"\n",
    "ids   = tokenizer(_test, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**ids, max_new_tokens=12)\n",
    "print(\"ðŸŸ¢ Generate OK:\", tokenizer.decode(out[0], skip_special_tokens=True)[:120])\n",
    "\n",
    "print(\"\\nâœ… Step 2 complete: model loaded in 4-bit and LoRA attached. Ready for Step 3 (dataset).\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619,
     "referenced_widgets": [
      "341fe3ecac1146188c3d5e2b188cc9ea",
      "05da768cea164789a27bef7c8fef302c",
      "fef5f73e8bb74bc3b98f891d2a49a833",
      "56238b7fc08b47faaa3b6d25a3f3061d",
      "086723ac2440431e81a2b90ed6aa49e6",
      "96ebe81291eb439193c01e87d8692eac",
      "c51b39e47f1e46deb72be678517e03f1",
      "315304d75ee3446297d54f1956241d8b",
      "f0e0b66fcd7a4024ac6af0ff23806f8a",
      "411d1ce8e5734bb19cb6bfa78ff89b2c",
      "77ea864af71d4ec38a8d1c72437cbc14",
      "dc8d7324d6a74f3aa105c6df80d8389e",
      "1c8d73f0f6fd417a9c252564d49d4c60",
      "472f4155565b4abea2cd52bd062d597a",
      "f280dce5f8e1439c8ddd8a4ef2db8d8a",
      "cfaf93719d1047c390f964da24dde9b1",
      "90ef64761e2f4041bd4352e22797759e",
      "182f207002e2401c94be216d002ce7ab",
      "32f4700ad88040058fcd0c8d6ad07971",
      "93222fce696a49a1905fc77c5daf13c9",
      "b59a8b4b2f7f41d38858e1353aa3a76e",
      "f05eb9b766194b61986900a3a0f89920",
      "778a00e97f0642a8b731cf405ce59fe7",
      "4751abc6bd80486babd084aaf7ebd07d",
      "684e5b12f93e4faa8afab0ff11b7ad1a",
      "e4a1f4c0738c4605a759da20ffca7c7a",
      "c98845ca1ce94422860d524471082bf5",
      "04350cbc9dc4432c8a4bcd3ba5008ee2",
      "db8e76f37a864c1895df50bc7278a49f",
      "9e13abb1d13b4ae198366ea9eead308d",
      "0540a8594a5d4af4be3fe6e7fb7da75c",
      "250643b3120a422ca405d7cd9e83ac06",
      "bd0714908aea4fe48201e997a3bab383",
      "4ebd4a34fb4846649b94420f18dc6cee",
      "246e0e602c884ccdacced01f73600a7a",
      "f285f532f6774189a4237d040f27a115",
      "8d019180c55f4cbe9b82bcf26d399a56",
      "67b38b43178d449faacd04ec58543bfb",
      "5886d9c8b884492aaa10e6a1e210442d",
      "31bc25bb91a543ee97f6e255a16a69be",
      "2501ade54a024fa493b580248102c17b",
      "31cf38d63467412ea58a11d5a2e66dc5",
      "404edbea88f0447995d9c634ef259304",
      "8a4f50bde5904a0387b18cc70c8a8362",
      "8e28dfe762704b0cb99e05e59dd21ea5",
      "ae0c0f93396a48acb2d91aa06a0e02a8",
      "6eac7c7a4acb443eb24e8d9774ebef6f",
      "6cba0ab57eeb41f491da39d1e2f9153e",
      "29bbf865816a4ee29d263a8f7409edd0",
      "e113b058eb984846980514ff856840da",
      "e2f3149de2b3482bb8c913bf4ce3254e",
      "890c15c6789d4016b03d72826b0b8ddf",
      "f6ff7f1d6039447fbd085fddb16c74b9",
      "b00dbc024d734ba5b94221031b32bb80",
      "7d83a4051c894b988f29578152156910",
      "c298c2944088414f8913e3d93adb7e0c",
      "eda67124f7024b4ab95722edf2f0229c",
      "285879ec06744498be8cd71869abdb92",
      "e670e78873e44fb08b57755b622309de",
      "2791fec9146c4dfd809db84e042bd4e8",
      "54e52f25a0d442fbb1b3c4594d59a0eb",
      "0f8bf7b36f344f4cad70e7fe5fe802b1",
      "75d5774d1a4b41ae89ecb9206e82d165",
      "a367bf834c6b44118d61dd0c5cba94aa",
      "942907cbfaac4895bec791b9f7b4cd68",
      "666704d9c83f45fb9d336f798addf67c",
      "76c3a0cc88704887bbe96692735d6238",
      "48758ad5803d4d21be4d04ec99a1ea67",
      "c4a1cba38eaa4803a7f374e8f2ce75a6",
      "29f1a169b0cb45018c0345ba7a07fb6b",
      "47a05971cbc3462183625d2493c302ed",
      "bb87f7a0c476436f8954312640de81d0",
      "58882c79aeea454b9327afd900844aa7",
      "485a0a2400ed493dae4cc963f0c5e948",
      "fbe2b5a33e5c412f8131b0d49078d27f",
      "a8b74c7f409d453296dd388bd5d9bb8e",
      "cc36cff701f94d4482ceb0940893f9ad",
      "36ef602a8b6544148adf663d8672ac92",
      "91c179114ee747bfbc7eb4f4fa58d39e",
      "54a5ba896ba145658cb5e4ff2a821227",
      "1d84f124354042ccb5d30d8b64585c70",
      "0336b690db5642f1b118b6e914df7ad6",
      "40759f7c3a074d6d8f9eaed0f5b6628e",
      "6a3004c424aa45ad861de60f0586d279",
      "09c7aa619de44c90ba4eb4894e00b9d0",
      "97420460c9ed407a98d086b72642048c",
      "a2875f580cee4b0281c3b9892bf42772",
      "0da4825111054fe5968e585c98be11af"
     ]
    },
    "id": "YpbrSTW1gVmF",
    "outputId": "01517f45-1a37-409b-b08b-2eeba4565f1b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-322365838.py:4: UserWarning: WARNING: Unsloth should be imported before trl, transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/742 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 4,884,480 || all params: 139,400,064 || trainable%: 3.5039\n",
      "ðŸŸ¢ Generate OK: Classify the sentiment: I absolutely love this new feature!\n",
      "\n",
      "The new feature is called \"Sentiment Analysis\".\n",
      "\n",
      "âœ… Step 2 complete: model loaded in 4-bit and LoRA attached. Ready for Step 3 (dataset).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Load AG News and format into Supervised SFT text\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load AG News\n",
    "raw = load_dataset(\"ag_news\")\n",
    "\n",
    "train = raw[\"train\"]\n",
    "valid = raw[\"test\"]\n",
    "\n",
    "# Optional: limit size during testing\n",
    "train = train.select(range(min(12000, len(train))))\n",
    "valid = valid.select(range(min(2000, len(valid))))\n",
    "\n",
    "# Label mapping\n",
    "LABELS = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\",\n",
    "}\n",
    "\n",
    "EOS = tokenizer.eos_token\n",
    "\n",
    "# SFT Prompt Template\n",
    "PROMPT = \"\"\"You are a news classifier. Classify the news article into one of the following categories:\n",
    "World, Sports, Business, Sci/Tech.\n",
    "\n",
    "### News Article:\n",
    "{}\n",
    "\n",
    "### Instructions:\n",
    "Respond with exactly one category from the list.\n",
    "\n",
    "### Category:\n",
    "{}\"\"\"\n",
    "\n",
    "# Convert dataset to text format for SFTTrainer\n",
    "def to_sft(batch):\n",
    "    texts = batch[\"text\"]\n",
    "    labels = batch[\"label\"]\n",
    "    out = []\n",
    "    for t, y in zip(texts, labels):\n",
    "        gold = LABELS[int(y)]\n",
    "        out.append(PROMPT.format(t, gold) + EOS)\n",
    "    return {\"text\": out}\n",
    "\n",
    "train_sft = train.map(to_sft, batched=True, remove_columns=train.column_names)\n",
    "valid_sft = valid.map(to_sft, batched=True, remove_columns=valid.column_names)\n",
    "\n",
    "print(\"âœ… Example sample:\\n\")\n",
    "print(train_sft[\"text\"][0][:500])\n",
    "print(\"\\nâœ… Step 3 complete â€” dataset loaded & formatted.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512,
     "referenced_widgets": [
      "c32aeaa7714044108f0be1979ac8c576",
      "9f562c60a1594c65907333cef2cfe7a4",
      "856081525a024e698e176f948d7e6fbf",
      "72845eefe53a40a6b8dd897c9845d97d",
      "e5e1e3ef7aa242ccac7f17440f474639",
      "41b528e15e6a4c619cc7ccd36dcb02e6",
      "aa850b1dd5f143fdac14d76bc1ed999f",
      "11e0d9c522a84bc48d4972046bd940dd",
      "ece4c6d93b3c41baa67a7ecf1f048e0e",
      "1581ee3c6b4b41478dd6038f8452b118",
      "ca0a940d7de44f63b4c0cb1b8f61ce1b",
      "7e9cf45c08894f1db305bcb42e14340a",
      "ca2dbb88e45a4725b8810d3be25e81af",
      "f6d98340f2c441f0895aa9bb071931d8",
      "8586d721737f4f16b48f2655232caa69",
      "2ee24d8c2e184f359076caaaec707277",
      "bbb54f6ba0da47458216dfa80c967c0d",
      "4e0578c659bc4bf7939e37fc1b390e61",
      "90f833548b204682a60fb494b678f5da",
      "f1275c66551f45969ebd6aec3387019b",
      "969209d4c7ce4edc82223ce5aae8b636",
      "23c7a74a8e4a47bf8ce315e24c1a5b82",
      "4bb02f6dd53145128c4c4a1f3d836143",
      "33515da8b6a64a5e97866132428319a0",
      "f9f9ae6666004c6980caec99d5305d86",
      "0ff3f7099c054866ba19572c9400947d",
      "90d71b366b7041519c83d7b269d540e5",
      "d20fc6527a7f417aa0a83d008f42c6a3",
      "35ba5a2f30a947abab4d33eca7742825",
      "2917ab625e7742b092cf1d7a31ff7ffd",
      "f8bf628923fb4c858266a1237b768284",
      "b4438eac6362457bad313313af59b4a0",
      "864d41012360481f989dd4600c4e7fa3",
      "bb2c7babd9714e59bed36efdef567c6c",
      "79e278e3dbec4d06803be1bfcf038372",
      "edc572b4b7694976a18209cb15649ca2",
      "8d61090874744e57a17f3d6582a48d52",
      "94f4b4ee1ee2482ca8ddba44beb1d3ee",
      "a1c4f30d29ef42fa85354dcd0a0bf090",
      "852af28ff0024e10819746e949577e11",
      "7d0f61c102174181b93f811b0b5051a5",
      "e898da5c7b0e4b47b44d087add55fda2",
      "03613cca459f481790b5e5fa0c066c9d",
      "3f4704dc5d0142e0b34a56b1f65a65a0",
      "ffd8880a013749fe888ab8e0cd50587c",
      "eb3c3963240c4cc08d89c176ede18247",
      "a76f66b7333f4773bc8cf84f8d3d1d3a",
      "fe21036edb354c308aa9dc982f88070a",
      "2d3274dc3008474cb4336162bf2d3735",
      "7a94707091cd46dcbae6077be3cc0d09",
      "903e278d605b48ed94812b6543533036",
      "211208d1e635465d8f83e70612fd374e",
      "839f22769f6e4ce1b3dcabb5f020ca79",
      "b9b0258d6274471a82b8828300beacce",
      "d7bc77ced94c459293be125e2cb910f5",
      "6423264f58734c70b08525578f3a3bf7",
      "cdb7e773478e487e9cb2935d1fa9a1c7",
      "ec02a87570fc4e2a8d840565ea348c96",
      "cd131263f19b47bdba7767aff6e9beeb",
      "0c14c053e9f94312bf28044b1ee81926",
      "141da37b346542c2a152100ee0241a6f",
      "2cceaf6ef0bf4a4890dbebd486e18e00",
      "2549618b82f3495a8b0b721409cbea0a",
      "68528975bf1a432384dd4baff7c0e11a",
      "248ca5a5f7b843c59f6b8c895ec63f2f",
      "279df5453d724eaf8a9e6a91c00063ca",
      "6b953e8b72f5435c849e4c69b94b1e2c",
      "781b0ea9c0454a9eb57a3f2273806a28",
      "d6d1185bc36740bf81d5d4cc544565c6",
      "afc8969cdefc4927bb73fb3a09b9de8e",
      "3b66c881fcbb4882b2330c505eedae33",
      "b331ab3c921f4c56ad661bf5e3984138",
      "d673a073be184b799bc5f86c46b765be",
      "23ee9f994fed455490247b5bfda4b065",
      "45dc32105c834ee286b060509cfdb937",
      "e3874570252c444db55f20ba034f0a73",
      "a57c01f2975f4a868d775c51d4d6371d"
     ]
    },
    "id": "aIqNlvGegVjG",
    "outputId": "5d418e15-3ef2-4643-d0ff-b799d9aff932"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Example sample:\n",
      "\n",
      "You are a news classifier. Classify the news article into one of the following categories:\n",
      "World, Sports, Business, Sci/Tech.\n",
      "\n",
      "### News Article:\n",
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "### Instructions:\n",
      "Respond with exactly one category from the list.\n",
      "\n",
      "### Category:\n",
      "Business<|endoftext|>\n",
      "\n",
      "âœ… Step 3 complete â€” dataset loaded & formatted.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Build TrainingArguments + SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Try both eval_strategy / evaluation_strategy for compatibility\n",
    "try:\n",
    "    args = TrainingArguments(\n",
    "        output_dir = \"outputs_lora_agnews\",\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        warmup_ratio = 0.03,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        weight_decay = 0.05,\n",
    "        logging_steps = 10,\n",
    "\n",
    "        save_strategy = \"epoch\",\n",
    "        eval_strategy = \"epoch\",            # âœ… new TRL versions\n",
    "        # evaluation_strategy = \"epoch\",     # âœ… old HF versions\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        report_to = \"none\",\n",
    "    )\n",
    "except:\n",
    "    args = TrainingArguments(\n",
    "        output_dir = \"outputs_lora_agnews\",\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        warmup_ratio = 0.03,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        weight_decay = 0.05,\n",
    "        logging_steps = 10,\n",
    "\n",
    "        save_strategy = \"epoch\",\n",
    "        evaluation_strategy = \"epoch\",      # âœ… fallback\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        report_to = \"none\",\n",
    "    )\n",
    "\n",
    "\n",
    "# âœ… Build SFTTrainer (NO formatting_func needed because \"text\" field exists)\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_sft,\n",
    "    eval_dataset = valid_sft,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "    packing = False,              # âœ… safe, no dataset packing\n",
    "    args = args,\n",
    ")\n",
    "\n",
    "print(\"âœ… Step 4 complete â€” Trainer built successfully (training not started).\")\n",
    "print(\"Train batches:\", len(trainer.get_train_dataloader()))\n",
    "print(\"Eval batches :\", len(trainer.get_eval_dataloader()))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135,
     "referenced_widgets": [
      "948dfe9e50d143ea8c2b50afad3ce18a",
      "335935a7373d446a959e2becd93d8a49",
      "42f90194458c4f02bd9d0883d40a217d",
      "d80aacc501e64ee9a0559c110e0ded3b",
      "3dadd3d56d8241db8568b97bc853d2d0",
      "2aa0f49343de4d8a916f7d9aae7652d9",
      "41eab5a25ff049cc9574c374670957e9",
      "da350764604640da8dd7e1bd16c2af83",
      "39b4b65797dd4edd8ac75ba1ed949857",
      "7b3d5981423843baaa638a56a07ae7d6",
      "3ebe7457528a478eb6a1b6bfc79801de",
      "808df3ddfa054d0b87fadcc616eb3e1d",
      "69966a6119c9464f87b2d3539b425293",
      "a8029af9bae64a738fc4b7614d5518bc",
      "5404b6ea94b14330a0923f3dee6e75b6",
      "b3a85e55bd114778a416f4912bed5573",
      "205983b5e73b479c832093bc897dc6b6",
      "c3f06a9475f54fb2bffdf4b2d014e5e5",
      "535bcc155266473685c53e56789048d2",
      "e866000846e9446a96dd4a63b0212fbc",
      "c757b4be98d04a0ab2ab57a3be283fa7",
      "d837e8ec029f443d9e9bf591bda49ab5"
     ]
    },
    "id": "YhjRlQvYgVgC",
    "outputId": "e3418738-4309-41e0-8a1b-37f6e192b67a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 4 complete â€” Trainer built successfully (training not started).\n",
      "Train batches: 750\n",
      "Eval batches : 250\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Training\n",
    "from math import ceil\n",
    "import torch\n",
    "\n",
    "# Nice-to-have: show an estimate of total steps\n",
    "num_update_steps_per_epoch = ceil(len(trainer.get_train_dataloader()) / trainer.args.gradient_accumulation_steps)\n",
    "print(f\"Epochs: {trainer.args.num_train_epochs} | \"\n",
    "      f\"Batches/epoch: {len(trainer.get_train_dataloader())} | \"\n",
    "      f\"GradAccum: {trainer.args.gradient_accumulation_steps} | \"\n",
    "      f\"Updates/epoch: {num_update_steps_per_epoch}\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_state()  # keeps optimizer/scheduler etc. for potential resume\n",
    "\n",
    "print(\"\\nâœ… Training finished.\")\n",
    "print(\"Train result metrics:\", {k: v for k, v in train_result.metrics.items() if isinstance(v, (int, float))})\n",
    "\n",
    "# Quick GPU memory note\n",
    "if torch.cuda.is_available():\n",
    "    peak_gb = torch.cuda.max_memory_reserved() / 1e9\n",
    "    print(f\"ðŸ’¾ Peak reserved GPU memory: {peak_gb:.3f} GB\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "X4WAG_ChgVXT",
    "outputId": "b477b860-abd6-44dc-a0eb-fb3a882ad5b3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epochs: 1 | Batches/epoch: 750 | GradAccum: 1 | Updates/epoch: 750\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 08:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.662500</td>\n",
       "      <td>1.598731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "\n",
      "âœ… Training finished.\n",
      "Train result metrics: {'train_runtime': 537.2789, 'train_samples_per_second': 22.335, 'train_steps_per_second': 1.396, 'total_flos': 1337310344300544.0, 'train_loss': 1.7350418319702148, 'epoch': 1.0}\n",
      "ðŸ’¾ Peak reserved GPU memory: 7.669 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"Switching model to inference mode...\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def build_infer_prompt(article: str) -> str:\n",
    "    return f\"\"\"You are a news classifier. Classify the news article into one of the following categories:\n",
    "World, Sports, Business, Sci/Tech.\n",
    "\n",
    "### News Article:\n",
    "{article}\n",
    "\n",
    "### Instructions:\n",
    "Respond with exactly one category from the list.\n",
    "\n",
    "### Category:\n",
    "\"\"\"\n",
    "\n",
    "# Create an inference pipeline\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Test samples\n",
    "samples = [\n",
    "    \"The stock market rallied today as major tech companies posted strong earnings.\",\n",
    "    \"NASA confirmed a new exoplanet that may contain signs of life.\",\n",
    "    \"Cristiano Ronaldo scored a hat-trick as his team dominated in a 5-1 win.\",\n",
    "    \"The Prime Minister met with European leaders to discuss climate policies.\",\n",
    "]\n",
    "\n",
    "print(\"\\nâœ… Running quick evaluation:\")\n",
    "for s in samples:\n",
    "    prompt = build_infer_prompt(s)\n",
    "    out = gen(prompt, max_new_tokens=6, do_sample=False)[0][\"generated_text\"]\n",
    "    prediction = out.split(\"### Category:\")[-1].strip()\n",
    "    print(\"\\nArticle:\", s)\n",
    "    print(\"Predicted:\", prediction)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DERbr-KjkJvO",
    "outputId": "ce1ccdc3-9e98-405d-ad30-b83a550787f2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Switching model to inference mode...\n",
      "\n",
      "âœ… Running quick evaluation:\n",
      "\n",
      "Article: The stock market rallied today as major tech companies posted strong earnings.\n",
      "Predicted: Business\n",
      "\n",
      "Article: NASA confirmed a new exoplanet that may contain signs of life.\n",
      "Predicted: Sci/Tech\n",
      "\n",
      "Article: Cristiano Ronaldo scored a hat-trick as his team dominated in a 5-1 win.\n",
      "Predicted: Sports\n",
      "\n",
      "Article: The Prime Minister met with European leaders to discuss climate policies.\n",
      "Predicted: World\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# âœ… Save LoRA adapter + tokenizer locally in Colab\n",
    "\n",
    "save_dir = \"/content/SmolLM2-135M-AGNews-LoRA\"\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"âœ… Model + tokenizer saved locally at:\", save_dir)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qE2ROITOkX_L",
    "outputId": "1fc9eed4-950f-47af-8e14-cdb375c33c13"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Model + tokenizer saved locally at: /content/SmolLM2-135M-AGNews-LoRA\n"
     ]
    }
   ]
  }
 ]
}