{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip -q install --upgrade \\\n",
    "  \"unsloth>=2025.10.0\" \\\n",
    "  \"transformers==4.57.1\" \\\n",
    "  \"accelerate>=1.10.0\" \\\n",
    "  \"trl>=0.23.0\" \\\n",
    "  \"peft>=0.17.1\" \\\n",
    "  \"bitsandbytes>=0.44.1\" \\\n",
    "  \"sentencepiece\"\n",
    "\n",
    "# IMPORTANT: import unsloth BEFORE transformers/datasets/peft so its patches take effect\n",
    "import os, sys, platform, json, torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import transformers, datasets, accelerate, trl, peft\n",
    "\n",
    "print(\"Python       :\", sys.version.split()[0])\n",
    "print(\"Platform     :\", platform.platform())\n",
    "print(\"Torch        :\", torch.__version__)\n",
    "print(\"Transformers :\", transformers.__version__)\n",
    "print(\"TRL          :\", trl.__version__)\n",
    "print(\"PEFT         :\", peft.__version__)\n",
    "print(\"Datasets     :\", datasets.__version__)\n",
    "\n",
    "# GPU check\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA GPU     :\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA GPU     : NOT AVAILABLE â€” training will be slow.\")\n",
    "\n",
    "# Load smollm2-135m in 4-bit\n",
    "model_name = \"unsloth/smollm2-135m\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = model_name,\n",
    "    max_seq_length = 2048,\n",
    "    dtype          = torch.float16,\n",
    "    load_in_4bit   = True,\n",
    "    device_map     = \"auto\",\n",
    ")\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Tiny generate smoke test\n",
    "prompt = \"You are a helpful assistant. In one sentence, say hello.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=24)\n",
    "\n",
    "print(\"\\n=== Smoke test output ===\")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "print(\"\\nâœ… Step 1 complete: environment OK and model loaded.\")\n",
    "\n",
    "DATASET_CHOICE = \"Dahoas/full-hh-rlhf\"\n",
    "print(\"Planned dataset:\", DATASET_CHOICE)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780,
     "referenced_widgets": [
      "be6dd63b3efe4adca91e6a05c9a161d6",
      "669d154ef7e64cf982b36d7feeaaec12",
      "36294eabe8944dccaec121a12d768f42",
      "8b55865c18da4421a5d9962d89991771",
      "6ca093e467dd41358569326ba2a0316a",
      "517e0a84aa07472bafc0a36ff863d3cd",
      "fa2d1f0672654a708b92aee32facb94f",
      "63aaf242d70948e98dc93319b604f25a",
      "d248e68fcafa4675b97f191e5ead76e3",
      "18aff7396355443ba9724a22e59bf548",
      "1321ef4df90c43c983911984d93fee58",
      "901948ef1587497fb43d46f635b0ca4a",
      "ef3436dd548c4835a664d6a2c27b4134",
      "4629e8c623744800bf1264a88908b922",
      "a3048cb43b034b8cae0b4d15f5cc8d6f",
      "2048515757ec4445960348471d92017e",
      "6055b19e64b341b98ec52e9dc8ef5f91",
      "b3bdc8e584674ec79fe716c7d8bce52e",
      "a0c3d97924f44d988bd4b16c74fac381",
      "bff85ca0ca094281b8a82534a63f58bf",
      "0180b08ed1ac4da29d25f5cebf900f25",
      "e988e28823b2485bbf2b2c9fc86408e1",
      "6cfeab9a48b34938ab77a428aa956cdf",
      "ae0adcf44c4e4527b594924d76f9571e",
      "5988cd4152974d30b113fedb1b31e526",
      "2522b7d91b384d8da9125c5d0f1a0c60",
      "1fa622814e8340c49f4d096e3d8eee5e",
      "e99d46a81415413094f56f1cab8e5575",
      "c1addbf44a194fd29eb2e0b2b8a2a8d4",
      "247947e32b5d4c2584c5d07ff8ac7501",
      "42945291760345e6858202a0fec83eba",
      "a25cd4aa80b245ef9bfff96a360d0c72",
      "01288706431c449b9b5184936516e503",
      "8d6dc649d3ef466bbcf317879c3266a2",
      "bef494ba7aac409cada717dcae6dc347",
      "35b6b7870f744a698d437c665fc53508",
      "83818e8638fb4222928527109aa1d965",
      "eb9b0aea18034aaf902e3d80e59fd926",
      "8590d0a7917e47fcb7a251d8be84d3b2",
      "00da346286fa4c8fac30bd967665b7b2",
      "6a1ca45303474d6595406b3ed49f5e3e",
      "8ec63b4fc9af462880f001e6f5fcfe5c",
      "2b90038ecd6a473992d5abfe6ae57c3a",
      "9b4078a83eff40afb4534647e4e6b6a0",
      "2e266b8f7f8546279acbdded3ac108c5",
      "3a9ebf98a893406bb0bdb9537f52995b",
      "33e6dbcfa9b548c8b99d70c7be1f8824",
      "c423ab40f7934c058889b5ad294752dc",
      "f1d032ed16fe40d1b7882ccefc63d5c6",
      "7f6eda22633e4fc4b2bea68c43386f05",
      "7f8bbc5ce92f47fea22dbe92a8fee1ec",
      "342c56176c1b4c90a2f312202f377966",
      "da675f75a6a749418ffe3945032e1510",
      "00330c300be84de2bc29f7abd7ea951e",
      "fa152be99ee44e8484e09a269b48e793",
      "a14e5987523045b2a6bf4ed737462fd5",
      "52c8c4b8fdd94892b80af84a203f386a",
      "4016738320c14772b8f734ef1e008c71",
      "18cc3f6760144927a732dc8dea51c156",
      "5f101da6a660428f8f712a97a4ab83bf",
      "7bb30b284c2540f4890fe3d564ea5348",
      "3794b5946d4e401989b9800e0177bd33",
      "8b21db1ddc9e44ccaf6a457b392d22ef",
      "a9a24e4cf4fd44e685c9b0313c61bffa",
      "a0ebcc055c17454ea10404ac529dbfff",
      "14d2f1da5bfd4697a5be41a605792a4e",
      "c5001d923ccf4908b751137616137fe1",
      "d4b829f76f304d1a8718539a80c81ed2",
      "4a06858c22c2449e927bacc337c2ac79",
      "bd1a8329058c4878a4d8243dfdd32468",
      "3027ca1642244380a4928b0ed39b9cb0",
      "d25ed65003db436f8b791210654e141b",
      "9dd529b43e4f4c7ca494d565d1d5d467",
      "3242860228694f7f85b900245096f3a0",
      "2b97a1e910e74decac8559220395a9bd",
      "b8bcecd98c9e4c2a82e3bf8f9f62b1b7",
      "43a280bc6d52447883b0c94d02291dcd",
      "009f4e96b2a3441092071575f4dcde28",
      "7488f32fa0074542bcb32babedf92008",
      "c50e600b086e46fcadeaaebedd6856cf",
      "a981ed89c24c42e18ac9ececd4cd1f2d",
      "5ddc3846815244d4b1d794cabd1d6923",
      "db38c17d4b2340648219057e6e5912ea",
      "57db0e7279e64ac480e5f255eb0fe3d2",
      "d490ce84082549be822034340a0dd572",
      "521026e0836a400e9bcb0a62f0d28e6b",
      "a39cb9cc2737482494e50eb22b21d457",
      "7848c491b4014822b8e979236d6a0506"
     ]
    },
    "id": "hNxUthf_c4tS",
    "outputId": "069904b2-9b1d-4b64-a390-9327813fdd28"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Python       : 3.12.12\n",
      "Platform     : Linux-6.6.105+-x86_64-with-glibc2.35\n",
      "Torch        : 2.8.0+cu126\n",
      "Transformers : 4.57.1\n",
      "TRL          : 0.25.0\n",
      "PEFT         : 0.17.1\n",
      "Datasets     : 4.3.0\n",
      "CUDA GPU     : Tesla T4\n",
      "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/742 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Smoke test output ===\n",
      "You are a helpful assistant. In one sentence, say hello.\n",
      "\n",
      "The other person is a good friend.\n",
      "\n",
      "The other person is a good friend.\n",
      "\n",
      "The other\n",
      "\n",
      "âœ… Step 1 complete: environment OK and model loaded.\n",
      "Planned dataset: Dahoas/full-hh-rlhf\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_CHOICE = \"Dahoas/full-hh-rlhf\"\n",
    "\n",
    "def load_preference_dataset(name: str, split: str = \"train\"):\n",
    "    \"\"\"\n",
    "    Returns a Dataset containing ONLY:\n",
    "        - prompt (str)\n",
    "        - chosen (str)\n",
    "        - rejected (str)\n",
    "    \"\"\"\n",
    "\n",
    "    if name == \"Dahoas/full-hh-rlhf\":\n",
    "        ds = load_dataset(\"Dahoas/full-hh-rlhf\", split=split)\n",
    "        keep = {\"prompt\", \"chosen\", \"rejected\"}\n",
    "        drop_cols = [c for c in ds.column_names if c not in keep]\n",
    "        if drop_cols:\n",
    "            ds = ds.remove_columns(drop_cols)\n",
    "        return ds\n",
    "\n",
    "    if name == \"Dahoas/synthetic-instruct-gptj-pairwise\":\n",
    "        ds = load_dataset(\"Dahoas/synthetic-instruct-gptj-pairwise\", split=split)\n",
    "        keep = {\"prompt\", \"chosen\", \"rejected\"}\n",
    "        drop_cols = [c for c in ds.column_names if c not in keep]\n",
    "        if drop_cols:\n",
    "            ds = ds.remove_columns(drop_cols)\n",
    "        return ds\n",
    "\n",
    "    if name == \"Anthropic/hh-rlhf\":\n",
    "        raw = load_dataset(\"Anthropic/hh-rlhf\", split=split)\n",
    "        # Anthropic dataset has NO explicit \"prompt\" field â†’ create empty prompt\n",
    "        def add_prompt(example):\n",
    "            example[\"prompt\"] = \"\"\n",
    "            return example\n",
    "        raw = raw.map(add_prompt)\n",
    "        keep = {\"prompt\", \"chosen\", \"rejected\"}\n",
    "        drop_cols = [c for c in raw.column_names if c not in keep]\n",
    "        if drop_cols:\n",
    "            raw = raw.remove_columns(drop_cols)\n",
    "        return raw\n",
    "\n",
    "    raise ValueError(f\"Unknown dataset name: {name}\")\n",
    "\n",
    "\n",
    "# --- Load the dataset ---\n",
    "train_dataset = load_preference_dataset(DATASET_CHOICE, split=\"train\")\n",
    "\n",
    "print(\"âœ… Dataset loaded successfully\")\n",
    "print(\"Dataset name :\", DATASET_CHOICE)\n",
    "print(\"Num rows     :\", len(train_dataset))\n",
    "print(\"Columns      :\", train_dataset.column_names)\n",
    "\n",
    "# --- Preview a sample row ---\n",
    "sample = train_dataset[0]\n",
    "\n",
    "def preview(text, n=280):\n",
    "    if isinstance(text, str) and len(text) > n:\n",
    "        return text[:n] + \" [...]\"\n",
    "    return text\n",
    "\n",
    "print(\"\\n--- SAMPLE ROW ---\")\n",
    "print(\"PROMPT  :\", preview(sample.get(\"prompt\")))\n",
    "print(\"CHOSEN  :\", preview(sample.get(\"chosen\")))\n",
    "print(\"REJECTED:\", preview(sample.get(\"rejected\")))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464,
     "referenced_widgets": [
      "9b2201d416f144f6987db3e117e14255",
      "245a84d288e8452bbd7c8cab513539fc",
      "1bca0b66be2b466d8a9119176698d0b2",
      "c22f87acf3254337b5657da4b22deb8b",
      "b41b1aae335a47139f27bec20b2ecaae",
      "79d232713f8d453bb41354769480a57f",
      "eb78a7f9e2374158bff1b45af8279563",
      "9f4336caf68642f1beb7f7b00b71e63d",
      "4af16fa826dd4c8fa15a59a7f826cb66",
      "89a291f1e0674327bb4bad94da7052ca",
      "ce80fd3b7d884ea4bff5a5d9d58057c9",
      "04e214820f484f31bc79d5b2e1db5b0a",
      "7f086f37232c48e9a279b12ecd27c26b",
      "d6d2e20627604cb4862c936a94bc6e62",
      "aa3eef9b0c7c4436bba0795ba03cba8e",
      "b7309e32c03645bc98536ac27ebe4ace",
      "34232449687d4e22b3aebb44642e9217",
      "cd68ba33c5404698a6704370bfd326f0",
      "42dc093cd2fb4acc832018db03491040",
      "b055a0bcf0ff4468ab6bf64797a72f51",
      "887432b25a2a4b13a41c28a24370c0dd",
      "4dcdb11a021240ce915f7452b222bf21",
      "3529f3807f2a42d6a33116e65d0485cc",
      "3e9782e574b448d2ae7a2209166f0829",
      "0a924623447141fe93324cc138bcc756",
      "f4e5b427b4ef4a8ebfbba9cadf885af2",
      "44e35ce9cd764b20a277b66e420d93d6",
      "953683a620354f04bca97fdf7f3e673a",
      "b8e1a7b349514d35bebd3b5c77239632",
      "b10d179e6e4a46a69dfaf73293e3ab7b",
      "e29fbba0656c4f3fb4acf05466a58396",
      "08202d735ccd43e6abbab1ac7688e7f4",
      "5405551177354531bafe849618daac74",
      "2ec95190bd7d4cffafaf1cf5b132bc8d",
      "77b2b926fc3041c2a64345c247841b24",
      "57440d62a72047019102daffc3ac7824",
      "70def0d9e2a74321826a1b716bfe14d4",
      "31b35085d49749408f96c435c78690d4",
      "1168a1daca35409a8484b18ee963e1ac",
      "8f6aae2e79114c2f84460e633dad6a4a",
      "0c9a981f6d0e425d8af6eb3dd94d0444",
      "4a123bb585ef48ca8ce6b42e398cef1b",
      "cb8fb3860852407ba4cc8ef3e24bd3b3",
      "bd5a00addb804070bdcd8dffda874a8f",
      "33f01fb39dd049f28ddfec7b4dc1dfe5",
      "7b745ab9ce0247f9ba9a40f9b40c3cdb",
      "06ebcdedf6354e9f88880794be5c8f60",
      "6657378cc579412dbb755e45467bf521",
      "d6460c35410543b799bbf7963abda26f",
      "8069a236ba1b463c958e0a3b48757560",
      "09d11a955b5c4ed08627e05ba9d505c8",
      "0d9af41130b64736a5c30ce42ffedaf2",
      "9d4f3ff703eb4116b01c57344db8dab4",
      "a3a4afe6831c4fabb2b22583b403066e",
      "115b4d329b354c349a4606efc2c40014",
      "e7a46f4426c645f09fbe60dbeb379e02",
      "0f7d8b6812d241d6985c031220d79d2d",
      "33db138d237946c8b42882eb1bec5263",
      "c865267ef49541c98a058d0d521475e4",
      "b12950319511480fbf9bc98fe68b4def",
      "9c16ed40f2d247c094444e0f4139e49c",
      "6e1a4861a5024a739c2139f0b5a6decc",
      "a7d28beee8f54926a850b4f55df79731",
      "02a89527333347dbad0b77965b795662",
      "1e9eafe572924380bbbcded2ede144b0",
      "a3948b7e18d14c8484e8066a5be82439"
     ]
    },
    "id": "K8Kit-WufiyD",
    "outputId": "af696ddc-2096-4747-fe6d-a23ee96506f5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/478 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001-8349d0765e6718(â€¦):   0%|          | 0.00/123M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/test-00000-of-00001-ec71e9262143a91(â€¦):   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/112052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/12451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Dataset loaded successfully\n",
      "Dataset name : Dahoas/full-hh-rlhf\n",
      "Num rows     : 112052\n",
      "Columns      : ['prompt', 'chosen', 'rejected']\n",
      "\n",
      "--- SAMPLE ROW ---\n",
      "PROMPT  : \n",
      "\n",
      "Human: Should you buy a case to protect your cell phone?\n",
      "\n",
      "Assistant: It depends on your circumstances.  If you carry your phone in a pocket or a purse then you probably want a case.  But if you only need a phone for quick interactions, a case may actually cause more harm than g [...]\n",
      "CHOSEN  :  Youâ€™re welcome.\n",
      "REJECTED:  It sounds like youâ€™ve got the basics down.  Any further questions or concerns?  You can send me any feedback you have at help@babba.ai.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenization\n",
    "def tokenize_pref(example):\n",
    "    \"\"\"\n",
    "    Creates model-ready input for DPO:\n",
    "        input_ids_chosen\n",
    "        attention_mask_chosen\n",
    "        input_ids_rejected\n",
    "        attention_mask_rejected\n",
    "    \"\"\"\n",
    "    prompt   = example[\"prompt\"]\n",
    "    chosen   = example[\"chosen\"]\n",
    "    rejected = example[\"rejected\"]\n",
    "\n",
    "    # Chat-style format (simple & safe for preference training)\n",
    "    text_chosen = f\"{prompt}\\n\\n### Response:\\n{chosen}\"\n",
    "    text_rejected = f\"{prompt}\\n\\n### Response:\\n{rejected}\"\n",
    "\n",
    "    # Tokenize separate paths\n",
    "    tokens_chosen = tokenizer(\n",
    "        text_chosen,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    tokens_rejected = tokenizer(\n",
    "        text_rejected,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "print(\"Tokenizing dataset... this may take ~1â€“2 minutes.\")\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_pref,\n",
    "    batched=False,\n",
    "    remove_columns=train_dataset.column_names,  # keep only tokenized fields\n",
    ")\n",
    "\n",
    "print(\"âœ… Tokenization complete!\")\n",
    "print(\"Columns now:\", tokenized_dataset.column_names)\n",
    "\n",
    "# Show 1 mini sample to confirm structure:\n",
    "sample_tok = tokenized_dataset[0]\n",
    "print(\"\\n--- TOKENIZED SAMPLE ---\")\n",
    "print(\"Chosen IDs length  :\", len(sample_tok[\"input_ids_chosen\"]))\n",
    "print(\"Rejected IDs length:\", len(sample_tok[\"input_ids_rejected\"]))\n",
    "print(\"Masks OK?          :\",\n",
    "      len(sample_tok[\"attention_mask_chosen\"]),\n",
    "      len(sample_tok[\"attention_mask_rejected\"]))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "f7f6102018df4cf69fa7b047f7332763",
      "d9a571f5c9f44b38ae408298399914d8",
      "c38a6a14b4da4b15846fae7a7f2069b4",
      "7289568a97ea4f37b4fe6dc90492968a",
      "ceca195ee7564ba09d851f7e7de2148a",
      "a1a6a5e203de4484a1e02710a68f96eb",
      "9141688897064d2da3d196504f8e0e79",
      "7aa6b621ba044097926cfa5345ed98fc",
      "06e5ac058aa14643aeabf254b0558d21",
      "955df4f0f21f433689ce0dd48d0c1f32",
      "976abb2b46f44bdf86f3cc87a075d2e6"
     ]
    },
    "id": "8IRE9_Qcfwek",
    "outputId": "a95dc40c-67d7-498b-e6b2-8eaae23d9813"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizing dataset... this may take ~1â€“2 minutes.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/112052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Tokenization complete!\n",
      "Columns now: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
      "\n",
      "--- TOKENIZED SAMPLE ---\n",
      "Chosen IDs length  : 512\n",
      "Rejected IDs length: 512\n",
      "Masks OK?          : 512 512\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Build DPOTrainer\n",
    "\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# 1) (Re)load RAW train split with the 3 required text columns\n",
    "train_dataset_raw = load_preference_dataset(DATASET_CHOICE, split=\"train\")\n",
    "print(\"RAW columns:\", train_dataset_raw.column_names[:10])\n",
    "\n",
    "# 2) Define LoRA (same as before â€” safe to re-run)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "        \"embed_tokens\",\"lm_head\",\n",
    "    ],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 3) Trainer args (turn off column pruning to be extra safe)\n",
    "dpo_config = DPOConfig(\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=0.02,         # tiny quick run; we can change later\n",
    "    output_dir=\"preference_rl_model\",\n",
    "    remove_unused_columns=False,    # <-- important\n",
    "    logging_steps=25,\n",
    ")\n",
    "\n",
    "# 4) Build trainer with the RAW dataset (NOT the tokenized one)\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=dpo_config,\n",
    "    train_dataset=train_dataset_raw,     # <-- raw\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… STEP 4 complete: Trainer is ready (built on RAW dataset).\")\n",
    "print(\"You can now run STEP 5 to start training.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314,
     "referenced_widgets": [
      "af3d317914dc463cba3fac75be083d28",
      "5eaa575290b547c9ae3ff8cc5cf4d6fd",
      "f441df484e32446c9e5d1d6abbc06205",
      "457c01f7b37848aab7abcb68fdbc5be7",
      "b27d79ffac274944937d09f21961492a",
      "bd2626e239944429a392a54e600d0a1c",
      "e594a1ae560b4d0d847d248bc0601a92",
      "83ea32420efa4f189cc0d213f941aaf0",
      "dac4aa7478a34251a0ee6fdb8adce0b6",
      "1b25c1248a1849d18369d5ef2e9e2ac0",
      "6980cae08b734b859d7cfd303be5b616",
      "dc26235cdad0441c85373fa94d106704",
      "d32bb1066d954e58a1528cbaa41aea6a",
      "592c4df619de46cb9bee2ed22a5c959b",
      "d2e2e411394146c4bb1358999a83075f",
      "f1dabf7ecaf04684b77b076774d8cc67",
      "6b6fc03197cf45d4b0bf217af3ae4a89",
      "6b43da5dc38645b780ecd1c140fd9157",
      "d20269ebb46647ac95a8aa39a6cea3f2",
      "5d75c2bb80d84be6a74b04fd46a7f5a6",
      "b53cddbb02f0443ba100c9af10a95693",
      "5ad097d05c454a389dc096902b57cd65",
      "0205d6faf02948e5a04b94add41a1640",
      "a96fe8d0444d49f9bed5cbce98240c1b",
      "3a10ebc52ab34752a964c47648ca2e4e",
      "ec363d1b1f0b4f80bc1bb4f1012332b6",
      "5605cd8e4fab4f439da8c0068cadf1f4",
      "dbcb5497094e47c984fce0e50055306f",
      "33b3cee8315144c0a00d631308090e0c",
      "a790d2c827914476b479685d40a45e53",
      "7606e946ef7042e4b4c06db21a8fc33b",
      "0c9a714d8ff24ad688ba727db99bff2b",
      "853cc3a318db44fca07ea92495c139c9"
     ]
    },
    "id": "dZq8VhJ6hARO",
    "outputId": "406fe774-af1d-4d63-ab74-1d87373d3735"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RAW columns: ['prompt', 'chosen', 'rejected']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Extracting prompt in train dataset (num_proc=6):   0%|          | 0/112052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Applying chat template to train dataset (num_proc=6):   0%|          | 0/112052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train dataset (num_proc=6):   0%|          | 0/112052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… STEP 4 complete: Trainer is ready (built on RAW dataset).\n",
      "You can now run STEP 5 to start training.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Training\n",
    "\n",
    "print(\"Starting preference-RL training...\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training finished!\")\n",
    "print(train_result)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "id": "uG0tIzKeiw4c",
    "outputId": "b3906132-ec83-4e78-ae73-d0febaa9d0f3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting preference-RL training...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 112,052 | Num Epochs = 1 | Total steps = 141\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 6,475,808 of 140,991,392 (4.59% trained)\n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrutujabhaskarrao\u001b[0m (\u001b[33mrutujabhaskarrao-san-jose-state-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251110_205324-9udy53am</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rutujabhaskarrao-san-jose-state-university/huggingface/runs/9udy53am' target=\"_blank\">misunderstood-eon-27</a></strong> to <a href='https://wandb.ai/rutujabhaskarrao-san-jose-state-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/rutujabhaskarrao-san-jose-state-university/huggingface' target=\"_blank\">https://wandb.ai/rutujabhaskarrao-san-jose-state-university/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/rutujabhaskarrao-san-jose-state-university/huggingface/runs/9udy53am' target=\"_blank\">https://wandb.ai/rutujabhaskarrao-san-jose-state-university/huggingface/runs/9udy53am</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 15:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>eval_logits / chosen</th>\n",
       "      <th>eval_logits / rejected</th>\n",
       "      <th>nll_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.692800</td>\n",
       "      <td>0.006299</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>-179.216049</td>\n",
       "      <td>-170.474731</td>\n",
       "      <td>6.397267</td>\n",
       "      <td>6.435376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.044888</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.012374</td>\n",
       "      <td>-210.347260</td>\n",
       "      <td>-195.908737</td>\n",
       "      <td>6.311908</td>\n",
       "      <td>6.436098</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.091843</td>\n",
       "      <td>0.068295</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.023548</td>\n",
       "      <td>-215.496735</td>\n",
       "      <td>-194.208237</td>\n",
       "      <td>6.060249</td>\n",
       "      <td>6.013487</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>0.124838</td>\n",
       "      <td>0.091641</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.033197</td>\n",
       "      <td>-206.066757</td>\n",
       "      <td>-180.811508</td>\n",
       "      <td>6.312346</td>\n",
       "      <td>6.400402</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.141903</td>\n",
       "      <td>0.100805</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.041098</td>\n",
       "      <td>-203.565033</td>\n",
       "      <td>-178.278900</td>\n",
       "      <td>5.980382</td>\n",
       "      <td>6.306888</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:270: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… Training finished!\n",
      "TrainOutput(global_step=141, training_loss=0.6823462695940167, metrics={'train_runtime': 965.9835, 'train_samples_per_second': 2.32, 'train_steps_per_second': 0.146, 'total_flos': 0.0, 'train_loss': 0.6823462695940167, 'epoch': 0.020133509442044766})\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# merge to FP16\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import torch, os\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name   = \"unsloth/smollm2-135m\"\n",
    "ADAPTER_DIR  = \"preference_rl_model\"         # where trainer saved the LoRA adapter/tokenizer\n",
    "MERGED_DIR   = \"preference_rl_model_merged\"  # new folder for merged FP16 weights\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"1) Load a fresh FP16 base model (NOT 4-bit)â€¦\")\n",
    "base_model, base_tok = FastLanguageModel.from_pretrained(\n",
    "    model_name     = model_name,\n",
    "    load_in_4bit   = False,            # <-- important\n",
    "    dtype          = torch.float16,    # real FP16 weights\n",
    "    device_map     = \"auto\",\n",
    ")\n",
    "\n",
    "print(\"2) Attach the LoRA adapter from trainingâ€¦\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "\n",
    "print(\"3) Merge LoRA into the base model and drop adapter hooksâ€¦\")\n",
    "merged_model = peft_model.merge_and_unload()   # produces a plain FP16 Transformers model\n",
    "\n",
    "print(\"4) Save merged model + tokenizerâ€¦\")\n",
    "merged_model.save_pretrained(MERGED_DIR, safe_serialization=False)\n",
    "base_tok.save_pretrained(MERGED_DIR)\n",
    "print(\"âœ… Merge complete ->\", MERGED_DIR)\n",
    "\n",
    "# ---------- Quick generation test (without reloading) ----------\n",
    "print(\"\\n>>> Quick generation with the in-memory merged model\")\n",
    "test_prompt = \"What is the difference between data science and machine learning?\"\n",
    "tok_inputs  = base_tok(test_prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "with torch.inference_mode():\n",
    "    out = merged_model.generate(**tok_inputs, max_new_tokens=120)\n",
    "print(base_tok.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# ---------- Optional: Reload from disk and test again ----------\n",
    "print(\"\\n>>> Reload merged model from disk for clean inference\")\n",
    "# Keep unsloth imported before transformers (done above). Now load normally:\n",
    "reload_tok = AutoTokenizer.from_pretrained(MERGED_DIR, use_fast=True)\n",
    "reload_mdl, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name     = MERGED_DIR,\n",
    "    load_in_4bit   = False,\n",
    "    dtype          = torch.float16,\n",
    "    device_map     = \"auto\",\n",
    ")\n",
    "with torch.inference_mode():\n",
    "    out2 = reload_mdl.generate(**reload_tok(test_prompt, return_tensors=\"pt\").to(reload_mdl.device),\n",
    "                               max_new_tokens=120)\n",
    "print(reload_tok.decode(out2[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nâœ… Step complete: merged FP16 model saved and inference verified.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8_5ZbaSoioq",
    "outputId": "d4e5ee6e-9741-4d69-a0d3-be0dfa697089"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1) Load a fresh FP16 base model (NOT 4-bit)â€¦\n",
      "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "2) Attach the LoRA adapter from trainingâ€¦\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3) Merge LoRA into the base model and drop adapter hooksâ€¦\n",
      "4) Save merged model + tokenizerâ€¦\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:424: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load original tied model\n",
      "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
      "\n",
      "# Set the randomly initialized lm_head to the previously tied embeddings\n",
      "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
      "\n",
      "# Save the untied model\n",
      "untied_model_dir = \"dir/for/untied/model\"\n",
      "model.save_pretrained(untied_model_dir)\n",
      "model.config.save_pretrained(untied_model_dir)\n",
      "\n",
      "# Now use the original model but in untied format\n",
      "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
      "```\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Merge complete -> preference_rl_model_merged\n",
      "\n",
      ">>> Quick generation with the in-memory merged model\n",
      "What is the difference between data science and machine learning?\n",
      "\n",
      "Data science is the process of collecting, analyzing, and interpreting data to make predictions, decisions, and decisions. Machine learning is a subset of data science that focuses on developing algorithms that can learn from data without being explicitly programmed.\n",
      "\n",
      "What is the difference between data science and machine learning?\n",
      "\n",
      "Data science is the process of collecting, analyzing, and interpreting data to make predictions, decisions, and decisions. Machine learning is the process of developing algorithms that can learn from data without being explicitly programmed.\n",
      "\n",
      "What is the difference between data science and machine learning?\n",
      "\n",
      "Data science is\n",
      "\n",
      ">>> Reload merged model from disk for clean inference\n",
      "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "What is the difference between data science and machine learning?\n",
      "\n",
      "Data science is the process of collecting, analyzing, and interpreting data to make predictions, decisions, and decisions. Machine learning is a subset of data science that focuses on developing algorithms that can learn from data without being explicitly programmed.\n",
      "\n",
      "What is the difference between data science and machine learning?\n",
      "\n",
      "Data science is the process of collecting, analyzing, and interpreting data to make predictions, decisions, and decisions. Machine learning is the process of developing algorithms that can learn from data without being explicitly programmed.\n",
      "\n",
      "What is the difference between data science and machine learning?\n",
      "\n",
      "Data science is\n",
      "\n",
      "âœ… Step complete: merged FP16 model saved and inference verified.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Tiny evaluation on preference pairs\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch, math, random\n",
    "from datasets import load_dataset\n",
    "\n",
    "MERGED_DIR  = \"preference_rl_model_merged\"   # from previous step\n",
    "DEVICE_MAP  = \"auto\"\n",
    "DTYPE       = torch.float16\n",
    "MAXLEN      = 512\n",
    "N_EVAL      = 200          # try 50 for a very quick run; increase for better signal\n",
    "SEED        = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# 1) Load merged FP16 model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name    = MERGED_DIR,\n",
    "    load_in_4bit  = False,\n",
    "    dtype         = DTYPE,\n",
    "    device_map    = DEVICE_MAP,\n",
    ")\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 2) Load a test split with (prompt, chosen, rejected)\n",
    "test_raw = load_dataset(\"Dahoas/full-hh-rlhf\", split=\"test\")\n",
    "keep = {\"prompt\",\"chosen\",\"rejected\"}\n",
    "drop_cols = [c for c in test_raw.column_names if c not in keep]\n",
    "if drop_cols: test_raw = test_raw.remove_columns(drop_cols)\n",
    "\n",
    "# Subsample for speed\n",
    "if len(test_raw) > N_EVAL:\n",
    "    test_raw = test_raw.shuffle(seed=SEED).select(range(N_EVAL))\n",
    "\n",
    "def build_pair_text(example):\n",
    "    p = example[\"prompt\"] or \"\"\n",
    "    return (\n",
    "        f\"{p}\\n\\n### Response:\\n{example['chosen']}\",\n",
    "        f\"{p}\\n\\n### Response:\\n{example['rejected']}\",\n",
    "    )\n",
    "\n",
    "@torch.inference_mode()\n",
    "def mean_logprob_of_response(full_text: str, prompt_len_tokens: int):\n",
    "    \"\"\"Compute mean token logprob of the continuation given the prompt.\n",
    "       We tokenize once, shift labels, and average continuation token logprobs.\"\"\"\n",
    "    enc = tokenizer(full_text, return_tensors=\"pt\", truncation=True, padding=False, max_length=MAXLEN)\n",
    "    input_ids = enc[\"input_ids\"].to(model.device)\n",
    "    attn_mask = enc[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    out = model(input_ids=input_ids, attention_mask=attn_mask, labels=input_ids)\n",
    "    # token-wise negative log-likelihood\n",
    "    # out.loss is averaged over all tokens; we want continuation only\n",
    "    # So compute per-token loss from logits\n",
    "    logits = out.logits[:, :-1, :]                      # (B, T-1, V)\n",
    "    labels = input_ids[:, 1:]                           # (B, T-1)\n",
    "    # cross-entropy per token\n",
    "    logprobs = torch.log_softmax(logits, dim=-1).gather(-1, labels.unsqueeze(-1)).squeeze(-1)  # (B, T-1)\n",
    "\n",
    "    # mask: only continuation tokens (after prompt_len_tokens)\n",
    "    Tm1 = logprobs.size(1)\n",
    "    cont_start = max(1, min(prompt_len_tokens, Tm1))    # avoid OOB\n",
    "    cont_mask = torch.zeros_like(logprobs, dtype=torch.bool)\n",
    "    cont_mask[:, cont_start:] = True\n",
    "\n",
    "    sel = logprobs.masked_select(cont_mask)\n",
    "    if sel.numel() == 0:\n",
    "        return -1e9  # no continuation, treat as very low score\n",
    "    return sel.mean().item()\n",
    "\n",
    "def prompt_len_tokens_from_prompt(prompt_text: str):\n",
    "    ids = tokenizer(prompt_text, truncation=True, max_length=MAXLEN)[\"input_ids\"]\n",
    "    return len(ids)\n",
    "\n",
    "# 3) Evaluate win-rate: logprob(chosen) > logprob(rejected)\n",
    "wins = 0\n",
    "rows = []\n",
    "for ex in test_raw:\n",
    "    chosen_text, rejected_text = build_pair_text(ex)\n",
    "    prompt_tokens = prompt_len_tokens_from_prompt(ex[\"prompt\"] or \"\")\n",
    "    lp_c = mean_logprob_of_response(chosen_text,   prompt_tokens)\n",
    "    lp_r = mean_logprob_of_response(rejected_text, prompt_tokens)\n",
    "    win = 1 if lp_c > lp_r else 0\n",
    "    wins += win\n",
    "    # keep a few examples for display\n",
    "    if len(rows) < 5:\n",
    "        rows.append({\n",
    "            \"lp_chosen\": round(lp_c, 4),\n",
    "            \"lp_rejected\": round(lp_r, 4),\n",
    "            \"margin\": round(lp_c - lp_r, 4),\n",
    "            \"snippet_prompt\": (ex[\"prompt\"] or \"\")[:100].replace(\"\\n\",\" \"),\n",
    "            \"snippet_chosen\": ex[\"chosen\"][:100].replace(\"\\n\",\" \"),\n",
    "            \"snippet_rejected\": ex[\"rejected\"][:100].replace(\"\\n\",\" \"),\n",
    "        })\n",
    "\n",
    "win_rate = wins / len(test_raw)\n",
    "print(f\"\\nâœ… Eval complete on {len(test_raw)} pairs.\")\n",
    "print(f\"Win-rate (chosen beats rejected by mean logprob): {win_rate:.3f}\")\n",
    "\n",
    "print(\"\\n--- sample scored rows ---\")\n",
    "for i, r in enumerate(rows, 1):\n",
    "    print(f\"[{i}] margin={r['margin']:+.4f} | lp_c={r['lp_chosen']:.3f} | lp_r={r['lp_rejected']:.3f}\")\n",
    "    print(f\"    prompt   : {r['snippet_prompt']}\")\n",
    "    print(f\"    chosen   : {r['snippet_chosen']}\")\n",
    "    print(f\"    rejected : {r['snippet_rejected']}\\n\")\n",
    "\n",
    "# 4) Tiny interactive demo\n",
    "def generate(prompt: str, max_new_tokens: int = 128):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nTry me:\\n\", generate(\"Explain DPO in simple terms. Give a 3-sentence answer.\"))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPfjAAm-qZmW",
    "outputId": "fced43a9-ac2a-466f-98cb-5f64363318ee"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "âœ… Eval complete on 200 pairs.\n",
      "Win-rate (chosen beats rejected by mean logprob): 0.560\n",
      "\n",
      "--- sample scored rows ---\n",
      "[1] margin=-0.7119 | lp_c=-2.311 | lp_r=-1.599\n",
      "    prompt   :   Human: What are the planets of the solar system?  Assistant: The solar system consists of eight pl\n",
      "    chosen   :  Pluto was a planet for a long time, but more recently it was downgraded to the status of a dwarf pl\n",
      "    rejected :  Pluto was considered to be the ninth planet until 2006, but it was found that it is much smaller th\n",
      "\n",
      "[2] margin=+0.4102 | lp_c=-2.883 | lp_r=-3.293\n",
      "    prompt   :   Human: Do you know of a recipe for cilantro lime rice?  Assistant: Let me see, it sounds like you \n",
      "    chosen   :  Let me see if I can find an internet recipe for a food like that. Iâ€™ll\n",
      "    rejected :  Does this sound right to you?\n",
      "\n",
      "[3] margin=+0.3027 | lp_c=-3.356 | lp_r=-3.658\n",
      "    prompt   :   Human: I like to host guests at my home from time to time, and I am gathering  recipes of differen\n",
      "    chosen   :  you want to add all of those ingredients into a pan, and cook them together?\n",
      "    rejected :  Can you please check to make sure I got all of that correctly?\n",
      "\n",
      "[4] margin=-0.0059 | lp_c=-2.562 | lp_r=-2.557\n",
      "    prompt   :   Human: How do I get better at kicking with power, for soccer?  Assistant: A kick with power is an \n",
      "    chosen   :  Have you tried practicing with some other people, to see what makes your kick go in a particular di\n",
      "    rejected :  Sure, you could also try practicing with a wall or a wall to wall doorway to get used to kicking of\n",
      "\n",
      "[5] margin=-0.2988 | lp_c=-2.537 | lp_r=-2.238\n",
      "    prompt   :   Human: Teaching kids how to program using Arduino  Assistant: Does that sound like something youâ€™d\n",
      "    chosen   :  If you tell me what youâ€™re currently looking for in an AI assistant, I can get you more specific ad\n",
      "    rejected :  Can you tell me a little more about this?  Are you doing it for a research project?  Or do you just\n",
      "\n",
      "\n",
      "Try me:\n",
      " Explain DPO in simple terms. Give a 3-sentence answer.\n",
      "\n",
      "DPO is a type of data compression. It is a way of storing data in a way that it can be easily read and understood by the computer. DPO is used in the data storage industry.\n",
      "\n",
      "DPO is a type of data compression. It is a way of storing data in a way that it can be easily read and understood by the computer. DPO is used in the data storage industry.\n",
      "\n",
      "DPO is a type of data compression. It is a way of storing data in a way that it can be easily read and understood by the computer. DPO is used in the data storage industry\n"
     ]
    }
   ]
  }
 ]
}